\documentclass[twoside,a4paper,article]{combine}
%\documentclass[12pt,twoside,a4paper]{article}

% =========================================================================
\usepackage[latin1]{inputenc}
\usepackage{a4}
\usepackage{fancyhdr}   
%\usepackage{german}    % Uncomment this iff you're writing the report in German
\usepackage{makeidx}
\usepackage{color}
\usepackage{t1enc}		% 	german letters in the "\hyphenation" - command
\usepackage{latexsym}	% math symbols
\usepackage{amssymb}    % AMS symbol fonts for LaTeX.
\usepackage{amsmath} 
\usepackage{physics}
\usepackage{graphicx}
\usepackage{pslatex}
\usepackage{ifthen}
\usepackage{tcolorbox}

\usepackage[T1]{fontenc}
\usepackage{pslatex}

\usepackage{psfrag}
%\usepackage{subfigure}
\usepackage{subcaption} 	
\usepackage{url}

% =========================================================================

\setlength{\oddsidemargin}{3.6pt}
\setlength{\evensidemargin}{22.6pt}
\setlength{\textwidth}{426.8pt}
\setlength{\textheight}{654.4pt}
\setlength{\headsep}{18pt}
\setlength{\headheight}{15pt}
\setlength{\topmargin}{-41.7pt}
\setlength{\topskip}{10pt}
\setlength{\footskip}{42pt}

\setlength{\parindent}{0pt}

% =========================================================================

\graphicspath{
	{pictures/}
}

%%%
% We want also subsubsections to be enumerated
%%%
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\makeglossary
%\makeindex

% =========================================================================
\begin{document}

\include{titlepage}

\begin{abstract}
% +++++++++++++++++++++++++
% Insert your Abstract here
In this study, we simulated a quantum annealing process at zero and finite temperature by using full diagonalisation and the Suzuki-Trotter product formula approach. We used quantum annealing to solve the 2-satisfiability ( 2-SAT) problem. We have demonstrated that the probability for finding the solution depends not only on the total annealing time, the minimum gap between the ground state and the first excited state of the system during the annealing process, but also on the temperature.
% +++++++++++++++++++++++++
\end{abstract}

\tableofcontents
\newpage
% =========================================================================

\section{Introduction}
%\cite{Ladd2010} General Possible Quantum Computer. 
%\cite{Boixo2016} The supremacy
%\cite{Ronnow2014} Detect quantum speed up

The idea of quantum computation was brought up in the early 80s \cite{Feynman1982}. Feynman thought that a simulation of quantum phenomena is not always possible on a classical computer, because the amount of resources required for the computation grows exponentially with the size of the physical system. Instead, quantum phenomena should be simulated by a computer making use of quantum properties. The amount of resources required for the simulation is only proportional to the size of the physical system. \\

After this idea, an interesting topic for scientists is that computation can benefit from storing, transferring and processing information with quantum properties. Quantum computation is expected to do more than just simulation of quantum phenomena. Some quantum algorithms were presented and shown to have a significantly speed-up compared to classical ones. Here follows some examples. Simon's algorithm can reduce the runtime for the task from an exponential time on a classical computer to a polynomial time on a quantum computer. \cite{Simon1994}. Furthermore, this also serves as the stepping-stone to Shor's Algorithm. Shor's Algorithm for factoring large intergers is one of the most well-known quantum algorithms. The time it takes to factor an n-digit number grows as a polynomial in n on a quantum computer. The time for the same task grows exponentially with n on a classical computer \cite{Shor1995}. The toughness of the factoring problem is the basis of many cryptography techniques and information is usually encrypted and protected by a large semi prime number. Classical cryptography seems to break down and  quantum cryptography is under research as a response to this. The last one is Grover's search algorithm. To search an item in an unstructured list of size N costs a classical computer running time on the order of N. Grover's algorithm can solve the same task in the order of $\sqrt{N}$ \cite{Grover1996}. Not like Shor's algorithm, which has an exponential speed-up, Grover's algorithm has only a quadratic improvement. It is still an important algorithm because of its board applications, such as to speedup the time required to solve NP-complete problems \cite{Cerf2000,Bennett1997}. \\

All algorithms mentioned above are expected to run on a universal quantum computer, which is usually referred to a machine based on the quantum gate model. However, building a quantum gate computer may be a challenging task. The main difficulty comes from the close box requirement \cite{Ladd2010}. The quantum system of a quantum computer should be isolated from its environment while being controlled. The quantum system is so fragile that even a little amount of noise can cause harm to the system, known as decoherence. Furthermore, due to the environmental noise, the entropy of the system will keep increasing with time. Therefore, there must exist a way to cool the quantum system and maintain its quantum state. How to measure results is another important issue, because the computational result makes sense to us only when we are able to measure it. Also, the possibility to scale up the system is crucial. A general quantum computer can be realised only when the above requirements are all fulfilled. Another approach to quantum computation is adiabatic quantum computer, which is more robust than a gate based computer !!cite decoherence in AQC!!. There are debates on whether an adiabatic quantum computer can be considered as a general quantum computer. It has shown that the model of adiabatic computation differs from the standard model of quantum computation within a polynomial time \cite{Aharonov2004,Farhi2000}. Some criticised that this only holds under ideal circumstance without noise. There are effective model for building a fault-tolerant gate based quantum computer. By contrast, whether fault-tolerant is possible on a quantum annealer is unknown !!cite Classical signature of quantum annealing!!\\

Quantum annealing can be considered as a special kind of quantum computation. Quantum annealer is a device that can perform quantum annealing. D-Wave Systems Inc. is a company which focuses on building a quantum annealer and the target is optimisation. Many application can be cast into a optimisation problem of finding the minimum of cost function, which is equivalent to finding the ground state of a system of interacting spins. Finding such state remains still a computationally hard problem. It is believed that if one can encode the cost function into the hardware, quantum annealing process can effectively achieve the goal\cite{Johnson2011}. During a quantum annealing process, the system starts with a ground state. At the end of the evolution, the system is expected to be in the ground state of the problem Hamiltonian, which is the solution of the optimisation problem. This is the reason why the quantum annealer is sometimes considered to be specialised in optimisation only instead of being a universal quantum computer. D-wave Systems was founded in 1999 and released the first commercial system in 2010 with 128 qubits. The latest system is D-Wave 2000Q with 2048 qubits. By emphasising on optimisation, a quantum annealer has potential application in fields like machine learning, pattern recognition and computer vision. \\

A quantum annealer is built based on a spin 1/2 system, which is Ising-model. The qubits in this system have two state. Start from an initial Hamiltonian, we slowly turn off initial Hamiltonian and turn on the problem Hamiltonian. If the system is in its ground state and the whole process is slow enough, the system will also end at ground state with the problem Hamiltonian and we get th e solution of the 2-SAT problem. According to the adiabatic theorem this should work in a pure system, however, in the real case with environment. The system will be coupled with a heat bath. This heat bath may damage the coherent of the system evolution. \\ 

The outline of the report is as follows. In section 2, the general idea of a quantum annealing and the theory behind it is introduced. Section 3 presents the definition and the characteristics of optimisation problems. Section 4 provides the algorithms used for the simulation of a spin-system. In Section 5 and section 6, the simulation results of a ideal system and a system with Temperature effect will be discuss respectively. Section 7 is about some applications on machine learning and computer vision based on quantum annealing.\\

% In section 3, I will introduce the optimization porblem, which believed to be a potential application of the quantum annealer. In section 4, I first simulate the quantum annealer without the heat bath. In section 5, I then simulate the quantum annealing process with hear bath. In the last section we try to discuss the possibility of using quantum annealing for machine learning.  \\

\newpage

\section{Theory of Quantum Annealing}

%!A quantum annealer is a specialised machine that solves optimisation problems by evolving from a know initial configuration to the ground state of a Hamiltonian encoding a given problem.
%\cite{Das2008} 
%\cite{Matsuda2009}
%\cite{Santoro2006}
%\cite{Denchev2015} what is the computational value of finite range tunnelling


\subsection{Quantum Annealing and Adiabatic Quantum Computation}
%\cite{Boixo2014} A compare of quantum annealing and simulated annealing \\
%\cite{QABoixo2016} Multi qubit tunneling \\
%\cite{Jin2013} quantum decoherence \\
%\cite{Smolin2013} Classical signature of quantum annealing p2 \\

Before going into quantum annealing, it's worth mentioning first the reason why using the annealing technique in general and what the difference between simulated annealing and quantum annealing. First, the reason for using the annealing technique are the properties of combinatorial optimisation problems. The goal of this kind of problem is to find an optimal solution among a finite set of possible solutions. The set of solutions is discrete. A brute-force search for the optimal one is not practical on a classical computer when the problem size is large. To reduce runtime, one approximate method is annealing. If one can encode the problem into a cost function of which the minimum value corresponds to the solution of the optimisation problem, an annealing algorithm can help to find the lowest point in the landscape constructed by the cost function.  \\

The annealing algorithm is inspired by the annealing in metallurgy and material science. It is a heat treatment that first heats up a material up to a temperature at which recrystallisation occurs, and then cools it down again. Recrystalisation processes lower the free energy of the crystals \cite{Schader2012}. It's an ancient technique to improve the properties of materials and make them more workable. \\

Following this process, simulated annealing is an optimisation method for approximating the global minimum of a given function. In simulated annealing, the system starts from a high energy state and by lowering the temperature very slowly, the system is expected to end up in the lowest energy state or at least a close approximation. However, there are two situations which may turn simulated annealing into an inefficient algorithm. First, if the landscape is too rugged and the energy barrier around local minima is too high, the system may get stuck in one of the local minima which is apparently not the optimal solution. These deep barriers may trap the system for a very long time during the evolution. The second problem is the complexity. The number of possible solutions grows exponentially with the problem size, because $n$ Ising spins have $2^n$ configurations. Simulated annealing can only go through one configuration at a time. Unless there is a gradient that can guide the system towards the global minimum from any point of the configuration space, the simulated annealing algorithm can do no better than a random search one. \\

On the other hand, quantum annealing may become more efficient and perform better than simulated annealing when facing these two situations, because the phenomenon of quantum tunnelling helps to explore the search space. More precisely, quantum mechanics allows the system to tunnel through very high barriers in a classically inaccessible path once barriers are narrow enough. Furthermore, if the Hamiltonian of a given problem is applied properly to the system, the quantum mechanical wave function can delocalise over the whole search space, that is, it has the ability to see the entire landscape during annealing. In contrast, simulated annealing can only search the configuration space randomly and escape from local minima by the aid of thermal fluctuations. Therefore, quantum annealing is superior to simulated annealing algorithm in these two aspects. However, empirically quantum annealing does not always improve the search process and it has its own limitations. Some counter cases have been observed for k-SAT problems \cite{Battaglia2005}. In brief, although for both algorithms the system may stop evolving at a local minimum state instead of the global minimum, if the barrier constraining the local minimum is narrow enough, the quantum tunnelling can give the system a chance to tunnel through the barrier and keep evolving towards the true minimum. An illustration is shown in Fig.\ref{diff_qa_ca}. \\ 

\begin{figure}[h]
	\centering
	\includegraphics[height=150pt]{Figure_cost_function.eps}
	\caption{The difference between quantum annealing and simulated annealing. }
	\label{diff_qa_ca}
\end{figure}

%The idea of annealing in general is to find the lowest-energy state, which sometime cast into a problem that try to find the lowest value of a cost function. When performing classic annealing, the system start from a random state in a high temperature. Then the temperature decrease, simulated algorithm choose the new state by some criteria. The system evolve by reducing the energy, and the result is usually end up in a local minima instead of global minima. !!The choose criteria\\

The main dynamics in the quantum annealing process is quantum fluctuation. To achieve this, the system is first prepared in the ground state of the initial Hamiltonian. Next, to bring in quantum fluctuations, a strong transverse field is applied to the system. During the annealing, the transverse field is slowly turned off while the problem Hamiltonian is slowly turned on. If this procedure progresses slowly enough, the system will stay still in the ground state. At the end of the annealing process, the transverse field is complete switched off and the system should have evolved to the ground state of the problem Hamiltonian that encodes the given optimisation problem. The Hamiltonian used here is the classical Ising model Hamiltonian. 
%and will be introduced in the next paragraph. How to encode an optimisation problem into a Ising model Hamiltonian will be presented in detail in Section \ref{The Mapping of Hamiltonian}. \\

%\begin{equation}
%H_{problem}=\sum_{i,j}^N \{J^x \sigma^x_i \sigma^x_j+J^y \sigma^y_i \sigma^y_j+J^z \sigma^z_i \sigma^z_j \}+ \sum_{i}^N \{h^x \sigma^x_i+h^y \sigma^y_i+h^z \sigma^z_i\}
%\end{equation}

%\begin{equation}
%H_{Ising}=\sum_{i,j}^N \{J^z \sigma^z_i \sigma^z_j \}+ \sum_{i}^N \{h^z \sigma^z_i\}
%\end{equation}

\begin{equation}
H_{Ising}=\sum_{i,j}^N \{J_{ij} \sigma_i \sigma_j \}+ \sum_{i}^N \{h_i \sigma_i\},
\end{equation}

where $\sigma_i$ stands for the Pauli matrices 

\begin{equation}
\label{pauli_matrix}
\begin{split}
&\sigma^x= \begin{pmatrix}
			0&1\\1&0
		  \end{pmatrix}\\
&\sigma^y=\begin{pmatrix}
			0&-i\\i&0
		  \end{pmatrix}\\
&\sigma^z=\begin{pmatrix}
			1&0\\0&-1
		 \end{pmatrix}.
\end{split}
\end{equation}

%!The Hamiltonian of classical Ising model with N spins can be written as above. 
The classical Ising model is broadly used in statistical physics to study phase transitions. The model describes a set of lattice sites with discrete variables that represent the atomic spins while in either up ($+1$) or down ($-1$). The model allows interaction between neighbours according to factor $J_{ij}$. If $J_{ij}<0$, the interaction is called ferromagnetic and the neighbour spins have a higher probability to align parallel. In contrast, if $J_{ij}>0$, it is antiferromagnetic and the neighbour spins favour opposite states. An external field $h_i$ interacts with each spin. The spin tends to have negative direction when $h_i>0$, and positive direction when $h_i<0$. Obviously, finding the spin configuration of the ground state given a parameter set is the main and difficult task after mapping an optimisation problem into the Hamiltonian. \\

Based on the classical Ising model, the Hamiltonian used in the quantum annealing process can be written as

\begin{equation}
\label{Hamiltonian_set}
\begin{split}
&H(t)=(1-\frac{t}{T} )H_{init}+(\frac{t}{T})H_{problem} \\
&H_{init}= \sum_{i=1}^{N}h_i^x \sigma_i^x\\
&H_{problem}= \sum_{i,j}^N J_{ij}^z \sigma^z_i \sigma^z_j + \sum_{i}^N h_i^z \sigma^z_i,\\
\end{split}
\end{equation} 

where t is the current timestep and T is total annealing time. It should be noted that by using $(1-\frac{t}{T})$ and $\frac{t}{T}$ as $H_{init}$ and $H_{problem}$ in Eq. (\ref{Hamiltonian_set}), it implies that the annealing proceeds with a linear time scheme. There are more general time schemes that can be implemented for the annealing process rather than a linear one. For the sake of simplicity, the linear time scheme is applied in the annealing process in this report. A illustration of coefficients for this shown in Figure \ref{linear_timescheme}.\\

The key feature of $H_{init}$ is that it should be straightforward to construct and its ground state is easy to find. A transverse field in the x-direction is the choice here. The ground of $H_{init}$ is $\ket{x_1=0}\ket{x_2=0}\ket{x_3=0}\dots\ket{x_n=0}$. This state can be written in our computational basis, the z-basis, in a superposition of all the $2^n$ basis vectors and the states $\ket{z_i}$ are eigenstates of the i-th spin in z-direction,

\begin{equation}
\ket{x_1=0}\ket{x_2=0}\ket{x_3=0}\dots\ket{x_n=0}=\frac{1}{2^{n/2}}\ket{z_1}\ket{z_2}\dots\ket{z_n}.
\end{equation}

%$H_{problem}$ is spanned by the computational basis, z-basis. 
The ground state of $H_{problem}$ can be found only when $z_1, z_2 \dots z_n$ in the superposition of the states $\ket{z_1}\ket{z_2}\dots\ket{z_n}$ can satisfy all clauses of the given optimisation problem. \\

%Quantum annealing does not evolve by reducing temperature. The system is affect by two Hamiltonian, one is $H_i$, which is our starting Hamiltonian, and the requirement of this Hamiltonian is that the ground state of this is easy to prepared. On the other hand, $H_p$ is the problem Hamiltonian. We map the problem in to $H_p$. The idea is that, if we start from the ground state of $H_i$, and then the $H_i$ is turned off and the $H_p$ is turned on sufficiently slow. The system will end up in the ground state of $H_p$, which is the solution we want to find.\\

At $t=0$ the annealing process starts in the ground state of $H_init$ and the quantum fluctuations dominate. According to the adiabatic theorem, which is introduced in section \ref{adiabatic theorem}, if the total annealing time is long enough, the system can evolve adiabatically and remain in the ground state. As a result, when $t=T$ and $H(t)$ shifts from $H_{init}$ to $H_{problem}$ completely, the final state is in the ground state of $H_{problem}$.  

\begin{figure}[h]
	\centering
	\includegraphics[height=150pt]{Figure_time_scheme.eps}
	\caption{Linear time scheme.  }
	\label{linear_timescheme}
\end{figure}

The adiabatic quantum annealing algorithm is presented as follow \cite{Farhi2000}:\\

\begin{tcolorbox}[title=Quantum Annealing Algorithm]
1. Initialise the system in the ground state of $H_{init}$. The ground state of $H_{init}$ is designed in a way that it is simple to construct.\\
%\begin{equation*}
%	H_i=\sum_{i=1}^{N}h_i^x \sigma_i^x
%\end{equation*}   

2. Set up $H_{problem}$ according to a given problem. Combine $H_{init}$ and $H_{problem}$ to build $H(t)$. \\
%\begin{equation*}
%	H_{problem}=\sum_{i,j} \{J^x \sigma^x_i \sigma^x_j+J^y \sigma^y_i \sigma^y_j+J^z \sigma^z_i \sigma^z_j \}+ \sum_{i} \{h^x \sigma^x_i+h^y \sigma^y_i+h^z \sigma^z_i\}
%\end{equation*}   

3. Evolve the system by computing time-dependent Schrödinger equation for time $t$ according to a given annealing scheme. \\
%\begin{equation*}
%	T:linear 
%\end{equation*}

4. The system will end up in the ground state of $H_p$, if the total annealing T is long enough.\\

%6. Measure the coefficient of $z_1,z_2 \dots z_n$. The result of the measurement will be the solution to the given problem, if it is satisfiable. If the given problem is not satisfiable, the system will end up in a configuration that minimise the number of violated clause.\\

\end{tcolorbox} 

It is worthwhile to mention the inherent difference between simulated annealing and quantum annealing. In the ideal case, simulated annealing has some random factor when choosing the starting point and also during the process, but quantum annealing does not have this randomness. Therefore, in principal, quantum annealing will either always find the solution, or never, which leads to a bimodal distribution. Classical annealing starting from a random state, on the other hand, may have a uni-modal result. This is considered as a main criterion to distinguish between simulated annealing and quantum annealing\cite{Boixo2013}. Based on this, D-wave Systems Inc. declared that there quantum annealer has quantum characteristics, but some opposite claimed that by picking the same random state, simulated annealing could also show a bimodal result \cite{Smolin2013}. \\

There are two crucial problems for quantum annealing. First, the length of the total annealing time T. How long should it be to solve an interesting problem? Second, when the given problem is complicated, the energy gap between each eigenstate may be very close. How would the gap between the ground state and first excited state effect the annealing process? These are two aspect that will be investigated in Section \ref{result_ideal} and Section \ref{result_temp}. \\
% Quantum annealing is a technique to find the ground state of a optimisation problem. different from the thermal annealing is that can tunnel through the barrier. 


\subsection{Adiabatic Theorem}
\label{adiabatic theorem} %\cite{Sarandy2004}

The adiabatic theorem mainly serves as the basis of quantum annealing. The concept is that if the Hamiltonian of an eigenstate alters gradually, then the state will remain an eigenstate at later times while the eigenenergy evolves continuously. The relation between gradually varying Hamiltonian and adiabatic behaviour can be demonstrated as follows. A quantum system that evolve it according to a time-dependent Schrödinger equation ($\hbar = 1$).

Equation \ref{tdse} states a quantum system which evolves unitarily.

\begin{equation}
\label{tdse}
-t\frac{\partial}{\partial t}\ket{\Psi (t)} = H(t)\ket{\Psi(t)},
\end{equation}

where $H(t)$ is the time-dependent Hamiltonian and $\ket{\Psi(t)}$ is a quantum state. Assume that the spectrum of $H(t)$ is nondegenerate. As a result, a basis can be defined as 

\begin{equation}
	H(t)\ket{n(t)}=E_n(t)\ket{n(t)},
\end{equation}

where $\ket{n(t)}$ is the set of eigenvector chosen to be orthonormal. Suppose that the time-dependent Hamiltonian can be diagonalised by unitary transformation  

\begin{equation}
\label{unitary}
H_D(t)=U^{-1}(t)H(t)U^{1}(t),
\end{equation} 

where $H_D(t)$ stands for the diagonalised Hamiltonian and $U(t)$ is its unitary transformation. By multiplying $U^{-1}$ to Eq (\ref{tdse}), it can be rewritten as 
 
\begin{equation}
H_D\ket{\Psi}_D = i \frac{\partial}{\partial t}\ket{\Psi}_D - i \frac{\partial}{\partial t} U^{-1} \ket{\Psi},
\end{equation}

where $\ket{\Psi}_D \equiv U^{-1}\ket{\Psi}$ is the state of the system in the basis of eigenvectors of $H(t)$. If $H(t)$ changes slowly enough in time, $\frac{\partial H(t)}{\partial t}$ approximates to zero. It can then be assumed that the unitary transformation, $U(t)$, and its inverse, $U^{-1}(t)$ are slowly varying operators, which leads to 

\begin{equation}
H_D(t)\ket{\Psi(t)}_D = i \frac{\partial}{\partial t} \ket{\Psi(t)}_D.
\end{equation}

Consequently, the system can evolve separetely in each energy eigenstate, because $H_d(t)$ is diagonal. In other words, this assures that if the quantum system is in an instantaneous eigenstate of $H(t)$ at a certain point in time, it will remain in this eigenstate at all time. \\ 

%Once the annealing time is long enough. We should be able to find the ground state.

\subsection{Landau-Zener Transition} %theorem

According to the Landau-Zener formula, another constraint on the quantum annealing process is the distance between the ground state and the first excited state. Landau-Zener formula gives the probability of a diabatic transition from a lower energy eigenstate to a higher energy eigenstate. Although the adiabatic theorem implies that no diabatic transition will occur if $H(t)$ in Eq. (\ref{Hamiltonian_set}) could vary infinitely slow, this is not the case for real systems with a finite annealing time. Once the system evolves in a finite annealing time from a lower energy eigenstate, the probability of finding the system in a higher energy state in the future can be determined by the Landau-Zener formula.\\

$P_{adiabatic}$ is the probability of the system remaining in the ground state and evolving adiabatically. $P_{diabatic}$ is the probability for a diabatic transition. The relation between these two is $P_{adiabatic} = 1 - P_{diabatic}$. According to the Landau-Zener formula, $P_{diabatic}$ is given by 

\begin{equation}
\begin{split}
P_{diabatic}(T) = \exp({\frac{-2\pi\Delta_{min}^2 T}{\alpha\Gamma_0}}),
\label{eq:landau}
\end{split}
\end{equation}

where $T$ is the total annealing time, $\Delta$ is the minimum gap between two energy eigenstates during the annealing process which are always referred to the gap between the ground state and the first excited state in this report, $\alpha$ is the relative slope of the two states, and $\Gamma_0$ is the initial strength of the transverse field which is set equal to $1$ in this report. An illustration of the Landau-Zener formula can be found in Figure. \ref{fig:landau}. \\

\begin{figure}[h]
	\centering
	\includegraphics[height=150pt]{Figure_Landau_Zener.eps}
	\caption{A graph showing the Landau-Zener formula.}
	\label{fig:landau}
\end{figure}

The probability of a successful quantum annealing is thus confined by the minimum gap, $\Delta_{min}$ and the annealing time $T$ correspondingly. The relation between the successful probability and $\Delta_{min}$ can indeed be investigated with a fixed annealing time $T$. Further details will be presented in Section \ref{result_ideal} and Section \ref{result_temp}. \\

%The probability of finding the ground state is depend on the temperature and the gap.
%\subsection{Annealing Time scheme}
%From the previous sector, we can noticed that the time scheme for $\lambda$ should be chosen and set. Here we use linear scheme for $\lambda$.

\newpage
\section{Optimisation Problems}
\subsection{Combinatorial Optimisation}

Combinatorial optimisation is one of the most important tasks people want to solve. If one needs to find the best choice among all available options, it can be considered as a combinatorial optimisation problem. Some practical examples are the travelling salesman problem, the closure problem and the assignment problem. The travelling salesman problem for instance, there are N cities located randomly in a given area and the goal is to plan a trip to visit every city once in the shortest travel distance. A straightforward approach is to try out all permutations and find the shortest path. The running time for this brute-force search is $\mathcal{O}(N!)$ , which means that it grows as the factorial of the number of cities and the search space easily becomes too large for this approach to be feasible even for relatively small N ($N \sim 30$) \cite{Santoro2006}. \\

Many combinatorial optimisation problems can be cast into a problem of minimising a given cost function $H(S_1,S_2,S_3, \dots S_N)$ with respect to N variables $S_1$, $S_2$, $\dots$ $S_N$ \cite{Das2008}. The aim is to determine a set of values for the variables that yield the minimum value of the cost function. Combinatorial optimisation often has the following properties. The optimal solution is searched from a finite set of objects. The set of objects is discrete. Brute-force search is usually not feasible. Besides, even some problems with continuous variables can be reduced to combinatorial problems \cite{Papadimitriou1984}. \\

In the field of computer science, the complexity of a computational task is classified by the running time needed by the fastest algorithm relative to the size of the task. If a given task can be solved in polynomial time by using polynomially bound resources on a classical computer, it is considered to be in the class P. Although a P problem does not necessary means an easy problem, a polynomial bound on the evaluation time implies its difficulty won't grow exponentially with the problem size. Unfortunately, not all cases fall in this class, and some of the important ones fall outside, like the travelling salesman problem mentioned above.\\

A given task is said to be in the class NP, if the solution can be found in polynomial time by a non-deterministic Turing machine. A deterministic Turing machine can only perform one action for a given situation according to its set of rules, while a non-deterministic Turing machine can perform more than one action for a given situation and if any of them finds the solution, it achieves its goal. In other words, a non-deterministic Turing machine has the potential to explore exponentially many paths in parallel with time and check if any on of them can solve the task. It's clear that a deterministic Turing machine is a weak version of a non-deterministic one. Therefore class P is also included inside class NP as a special case. Not like a problem in class P, for which a polynomial-time algorithm is known to exist, problems in class NP are believed to require super-polynomial time. \\

There is a subset inside class NP called NP-complete, which is considered to be the hardest set among class NP and any NP problem can be reduced into class NP-complete with a polynomial overhead. It is generally believed that if one can find a polynomial algorithm to solve an NP-complete problem, all problems in class NP can also be solved by this polynomial algorithm.  The tasks that fall inside either class NP or class NP-complete are considered to be hard, because a non-deterministic Turing machine cannot yet be simulated by a deterministic Turing machine without an exponential growth of execution time. \\ 

Indeed there are algorithms that can solve some easy optimisation problems in polynomial time. For harder cases like the one belonging to the class NP-complete, although one cannot find the exact solution in polynomial time, there exists some specialised algorithms that can approximate the solution in a polynomial time. The downside is that these algorithms are very problem specific, which means a success in one NP-complete problem does not ensure the success when attacking other NP problems by using the same algorithm. \\

\subsection{Boolean Satisfiability Problems}

The boolean satisfiability problem, abbreviated as the SAT problem, is a task of checking whether a given Boolean formula can be satisfied. To be more specific, the goal here is to find a configuration of true or false values for all variables in a given Boolean formula so that the final evaluation of the formula is true. The formula is called satisfiable when this is the case. On the other hand, the formula is not satisfiable, if the evaluation is false for all possible configurations.  \\

The k-SAT problem with k>2 has been proven to be in the class NP-complete, which implies that no algorithm can efficiently solve SAT problems in polynomial time. The parameter k defines the upper limit of the number of variables in one clause. An example is 

\begin{equation*}
 (x_1\lor x_2\lor x_3)\land (x_4\lor x_5\lor \neg x_6),
\end{equation*}

where $\land$,$\lor$, and $\neg$ state for logical and, logical or, and logical not respectively. This is a Boolean formula in 3-SAT form, which has two clauses and each clause consists of 3 variables. This formula is satisfiable because the final evaluation is true if one of the variables $x_1, x_2, x_3$ is true in the first clause ,and one of the variable $x_4, x_5$ or $\neg x_6$ is true in the second clause. \\

In this work, 2-SAT problems are considered In contrast to those more general ones which are known to be NP-complete, the 2-SAT problem can be solved in polynomial time. A detail that should be mentioned is that the 2-SAT problems studied in this work all have only one unique configuration which leads to a true evaluation.\\
 
\subsection{Mapping the 2-SAT Problem to the Ising Hamiltonian}
\label{The Mapping of Hamiltonian}

A 2-SAT problem can only be solved by quantum annealing when its Boolean formula is mapped to an Ising Hamiltonian. In the Ising model, a spin can take either value 1 or -1. It is straightforward to map spin value 1 to logical true and spin value 0 to logical false. \\

The following simple example can help demonstrates how a Boolean formula is mapped on the Ising Hamiltonian. Consider the Boolean formula 

\begin{equation*}
(x_1\lor x_2)\land (x_3\lor \neg x_4)
\end{equation*}

The whole statement is true if in the first clause, either $x_1$ or $x_2$ is true and in the second clause, $x_3$ is true or $x_4$ is false. The mapping is as follows:

\begin{table}[h!]



\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		
		\multicolumn{5}{c}{2-SAT Variables}\\
		\hline
		  & T & T & T & F \\
		\hline
		\hline
		$x_1$ & 1 & 1 & 0 & 0 \\
		\hline
		$x_2$ & 1 & 0 & 1 & 0 \\
		\hline
	\end{tabular}
	\quad
	$\Rightarrow$
	\quad
	\begin{tabular}{|c|c|c|c|c|}
		\multicolumn{5}{c}{Ising variables}\\
		\hline
		 & T & T & T & F \\
		\hline
		\hline
		$\sigma_1$ & 1 & 1 & -1 & -1 \\
		\hline
		$\sigma_2$ & 1 & -1 & 1 & -1 \\
		\hline
		$m=\sigma_1+\sigma_2$ & 2 & 0 & 0 & -2 \\
		\hline
	\end{tabular}
\end{center}

\begin{center}
	\begin{tabular}{|c|c|c|c|c|}

		\hline
		& T & T & T & F \\
		\hline
		\hline
		$x_3$ & 1 & 1 & 0 & 0 \\
		\hline
		$x_4$ & 0 & 1 & 0 & 1 \\
		\hline
		
	\end{tabular}
		\quad
		$\Rightarrow$
		\quad
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			& T & T & T & F \\
			\hline
			\hline
			$\sigma_3$ & 1 & 1 & -1 & -1 \\
			\hline
			$\sigma_4$ & -1 & 1 & -1 & 1 \\
			\hline
			$m=\sigma_3-\sigma_2$ & 2 & 0 & 0 & -2 \\
			\hline
		\end{tabular} 

\end{center}
\caption{Mapping a 2-SAT problem in term of the Ising variables.}
\end{table} 

In order to encode the solution of the 2-SAT problem into the ground state of the Ising Hamiltonian, an Ising Hamiltonian can be written down with a magnetisation $m$. The Hamiltonian with magnetisation $m$ is given by 

\begin{equation}
\begin{split}
H & = m \cdot (m-2)\\
  & = \sigma_i^2 + \sigma_j^2 + 2\sigma_i \sigma_j -2\sigma_i -2\sigma_j\\
  & = 2\sigma_i \sigma_j -2\sigma_i -2\sigma_j + const.\\
\end{split}
\end{equation}

After comparing with the Hamiltonian given in Eq. (??), one finds that $h_i^z$, $h_j^z$, and $J_{ij}$ correspond to 2, 2, and -2 respectively. Then one just repeats this procedure for all clauses. After this mapping is complete, the original 2-SAT problem can be solved with the quantum annealing technique, because the ground state of the Ising Hamiltonian can be transformed back to the solution of the 2-SAT problem. \\  



\newpage
\section {Simulation Algorithm}
\subsubsection{Time-Dependent Schrodinger Equation and Quantum Spin System}

The whole quantum annealing process can be described by time-dependent Schrödinger equation ( Equation \ref{tdse}), that is by solving time-dependent Schrödinger equation one can determine the final state of the system. The solution of this is 

\begin{equation}
\begin{split}
\Psi(t+\tau)&=U(t+\tau,t)\Psi(t)\\
&=\exp(i\int_{t}^{t+\tau}H(\tau)d\tau)\Psi(t),
\end{split}
\end{equation}

!where $\tau$ is the time step, and $U(t+\tau)$ is a unitary matrix that transform system from $t$ to $t+\tau$. In order to conduct numerical computation and ensure the unitary of the evolution operator, time is discretised to small time step $\tau$ and $H(\tau)$ is assumed to be piecewise constant within these time steps. Therefore, the size of the time step should be small enough to keep $H(\tau)$ piecewise. On the other hand, if the size is too small, the computational time will be too long. A proper length should be tested out before going into further simulation. \\

The $\Psi$ is defined by the direct product state of the N single spin states in z-basis, which is the computational basis used in this report. Each single spin has two possible states, $\ket{0}$ or $\ket{1}$, which correspond to, $\ket{\uparrow}$ or $\ket{\downarrow}$ . The relation between them is defined as 

\begin{equation}
\ket{0}=\ket{\uparrow}=\left(\begin{array}{c} 1\\0 \end{array}\right), \ket{1}=\ket{\downarrow}=\left(\begin{array}{c} 0\\1 \end{array}\right)
\end{equation}

Thus, the single spin state can be written as a linear superposition of theses two basis state, 

\begin{equation}
\ket{\psi} = a(0)\ket{0} + a(1)\ket{1},
\end{equation}

where $a(0) and a(1)$ are the coefficient of the amplitude of each state. Furthermore, a N-spin system is made up of $2^N$ state vectors and it can be written as 

\begin{equation}
\label{psi_in_compuationalbasis}
\Psi = a(00\dots0)\ket{00\dots0}+a(00\dots1)\ket{00\dots1}+\dots+a(11\dots0)\ket{11\dots0}+a(11\dots1)\ket{11\dots1},
\end{equation}

where $a(00\dots0), a(00\dots1)\dots, a(11\dots1)$ are the coefficients of each components. It should fulfil the normalisation requirement, $\braket{\Psi}{\Psi} = 1$. That is, the following equation should hold. 

\begin{equation}
\sum_{\sigma_1,\sigma_2, \dots, \sigma_N=\ket{0},\ket{1}} |a(\sigma_1,\sigma_2, \dots, \sigma_N)|^2 = 1. 
\end{equation}

One may noticed that the format of basis vectors in Equation \ref{psi_in_compuationalbasis} is similar to the format of binary number notation. !If the first is considered to be the least significat bit of an integer, the basis vector can be denoted as a integer ranging from 0 to $2^N-1$. \\
%According to Equation \ref{psi_in_compuationalbasis}, it is 

It is clear that the memory request for the computation will increase exponentially with the size of the quantum system, which may soon make the computation not practical on a classical computer. However, it turns out there is a simple way to calculate the result without accessing to the entire matrix every time. Namely, during applying $\sigma^x_i, \sigma^z_i, and \sigma^z_i\sigma^z_j$ to $\ket{\Psi}$ the computation can be reduced to series of $2x2$ and $4x4$ matrix calculation instead of a calculation of entire $2^N x 2^N$ matrix. After applying Hamiltonian operator to Equation \ref{psi_in_compuationalbasis}, the state vectors become 

\begin{equation}
\begin{split}
\ket{\Psi'} &= \sigma^\alpha_i \ket{\Psi} \\
			&= a'(00\dots0)\ket{00\dots0}+a'(00\dots1)\ket{00\dots1}+\dots+a'(11\dots0)\ket{11\dots0}+a'(11\dots1)\ket{11\dots1},
\end{split}
\end{equation}

If the operator is $\sigma^x_i$, it interchanges state up and state down, which corresponds to a swap of a pair component of $\Psi$. 

\begin{equation}
\begin{split}
a'(\bullet \cdots \bullet 0 \bullet \cdots \bullet) = +a(\bullet \cdots \bullet 1 \bullet \cdots \bullet)\\
a'(\bullet \cdots \bullet 1 \bullet \cdots \bullet) = +a(\bullet \cdots \bullet 0 \bullet \cdots \bullet)
\end{split}
\end{equation}

If the operator is $\sigma^y_i$, according to Equation \ref{pauli_matrix}, the coefficient has the relation 

\begin{equation}
\begin{split}
a'(\bullet \cdots \bullet 0 \bullet \cdots \bullet) = -i\times a(\bullet \cdots \bullet 1 \bullet \cdots \bullet)\\
a'(\bullet \cdots \bullet 1 \bullet \cdots \bullet) = +i\times a(\bullet \cdots \bullet 0 \bullet \cdots \bullet)
\end{split}
\end{equation}

If the operator is $\sigma^z_i$, it reverses the sign of all coefficients of $\ket{\Psi}$ for which the $i-th$ bit of the vector has value 1, which yields

\begin{equation}
\begin{split}
a'(\bullet \cdots \bullet 0 \bullet \cdots \bullet) = +a(\bullet \cdots \bullet 1 \bullet \cdots \bullet)\\
a'(\bullet \cdots \bullet 1 \bullet \cdots \bullet) = -a(\bullet \cdots \bullet 0 \bullet \cdots \bullet)
\end{split}
\end{equation}

Similarly, applying two spin operator $\sigma^z_i\sigma^z_j$ leads to a sign interchange between coefficients of $\ket{\Psi}$ for which the $i-th$ bit and the $j-th$ bit of the vector has different valus. Hence it is 

\begin{equation}
\begin{split}
a'(\bullet \cdots \bullet 0 \bullet \cdots \bullet 0 \bullet \cdots \bullet) = +a(\bullet \cdots \bullet 0 \bullet \cdots \bullet 0 \bullet \cdots \bullet)\\
a'(\bullet \cdots \bullet 1 \bullet \cdots \bullet 0 \bullet \cdots \bullet) = -a(\bullet \cdots \bullet 1 \bullet \cdots \bullet 0 \bullet \cdots \bullet)\\
a'(\bullet \cdots \bullet 0 \bullet \cdots \bullet 1 \bullet \cdots \bullet) = -a(\bullet \cdots \bullet 0 \bullet \cdots \bullet 1 \bullet \cdots \bullet)\\
a'(\bullet \cdots \bullet 1 \bullet \cdots \bullet 1 \bullet \cdots \bullet) = -a(\bullet \cdots \bullet 1 \bullet \cdots \bullet 1 \bullet \cdots \bullet)\\
\end{split}
\end{equation}

Please note that the operation mentioned here are done in place. Namely, instead of creating and using extra unnecessary vector, these operation manipulate the necessary component in the state vector $\ket{\Psi}$. Hence, $\ket{\Psi'}=H\ket{\Psi}$ can be calculated easily. This approach serves as the basis of the algorithm introduced in following subsections. \\
%\begin{equation}
%i\hbar\frac{\partial \psi}{\partial t}=H\psi
%\end{equation}
%\begin{equation*}
%H=(1-\lambda)H_{init}+\lambda H_{problem}
%\end{equation*}
%\begin{equation*}
%H_{init}=\sum_{i} h^x_{init} \sigma^x_i
%\end{equation*}


\subsection{Algorithm for Ideal Case}

So here i am going to try out two algorithm. First is full diagonalization, which i use Lapack to solve the eigensystem. Product formula break the evolution into small operation and make the computation of large system possible.\\


\subsubsection{Full Diagonalisation}

Base on the fact that Hamiltonian H is an Hermitian operator formed by a $2^N \times 2^N$ matrix, it has a complete set of eigenvectors and eigenvalues. This implies that the matrix H can be transformed into a diagonal matrix $\Lambda$ spanned by its eigenvalues and the unitary matrix $V$ of its eigenvectors. The transformation is given by $H = V\Lambda V^\dagger$. Thus the time evolution can be calculated as $U(\tau) = exp(-i\tau H) = V exp(-i\tau \Lambda) V^\dagger$. If $V$ and $\Lambda$ are obtained by diagonalising H, time evolution becomes a series of simple matrix multiplication. Since diagonalisation is a typical matrix problem, there are already standard software library can be made use of. \\

The element of H can be computed by repeatedly applying Hamiltonian H to basis vector $\ket{\Psi}$, that is $\ket{\Psi'} = H\ket{\Psi}$. ! First, $\ket{\Psi}$ is set to be $(1,0,\dots,0)^T$. $\ket{\Psi'} = H\ket{\Psi}$ will then result in the first column of the matrix H. Same calculation for  $(0,1,0,\dots,0)^T$ will lead to the second column of the matrix H and so forth. Consequently, the matrix H will be determined. \\

The main drawback of this approach is the limitation on scaling. !Whlie diagonalising the matrix H, the memory request and CPU time of the standard algorithm scale as $D^2$ and $D^3$ with $D=2^N$. Due to the exponential growth of the matrix size $D$, full diagonalisation approach fits only small quantum system. Therefore, this approach serves mostly as a tool to valid the correctness of other algorithm when solving with time-dependent Schrödinger equation\cite{DeRaedt2004}.\\
%Use Lapack to diagonalize the H for $\Psi_{t} = e^{iH\tau} \Psi_{t-\tau}$. 


\subsubsection{Suzuki-Trotter Product Formula}
Check Reference \cite{DeRaedt2004}  E. Suzuki-Trotter Product-Formula Algorithms

The Suzuki-Trotter product formula approach is the one that will suit for a larger quantum system. Suzuki-Trotter product formula approximate a unitary matrix exponentials by decomposing the matrix properly, that is 

\begin{equation}
\label{suzuki-trotter product formula}
\begin{split}
U(t) &= \exp(-i\tau H)\\
&=\exp(-t\tau(H_1+H_2+\dots+H_K))\\
&=\lim_{m \to \infty}(\prod_{k=1}^{K}\exp(-i\tau H_k/m))^m
\end{split}
\end{equation}

If the time step $\tau$ is small enough, a good approximation of $U(t)$ suggested by Equation \ref{suzuki-trotter product formula} is 

\begin{equation}
\label{suzuki-trotter product formula 1st}
\tilde{U}_1(\tau) = \exp(-i\tau H_1) \exp(-t\tau H_2) \dots \exp(-t\tau H_K).
\end{equation} 
 
 !The Taylor series of $U(t)$ and $\tilde{U}_1(t)$ are identical up to first order in $\tau$. Thus $\tilde{U}_1(t)$ is the first order approximation of $U(t)$. Furthermore, if all $H_i$ in Equation \ref{suzuki-trotter product formula 1st} are Hermitian, $\tilde{U}_1(t)$ is unitary by construction and hence the algorithm based on Equation \ref{suzuki-trotter product formula 1st} will be unconditionally stable. The accuracy of the algorithm can be increased by applying second order approximation and it is defined by 
 
\begin{equation}
\begin{split}
\tilde{U}_2(\tau) &= \tilde{U}_1(\frac{\tau}{2})^T\tilde{U}_1(\frac{\tau}{2}) \\
&=\exp(\frac{-t\tau H_K}{2})\exp(\frac{-t\tau H_{K-1}}{2})\dots\exp(\frac{-t\tau H_1}{2})\exp(\frac{-t\tau H_1}{2})\dots\exp(\frac{-t\tau H_K}{2})
\end{split}
\end{equation} 

Here $\tilde{U}_2(\tau)$ is again unitary by construction as $\tilde{U}_1(\tau)$. !For a fixed accuracy in this $\tilde{U}_2(\tau)$ approximation, memory required and CPU time scale as $\order{D}$ and $\order{\tau^{1+1/2}D}$ respectively. The crucial step of this algorithm is to choose $H_i$ properly in a way that it is easy enough to calculate there matrix exponential, $\exp(\frac{-t\tau H_1}{2})\dots\exp(\frac{-t\tau H_K}{2})$, efficiently.  \\

The Hamiltonian used in this report is listed in Equation \ref{Hamiltonian_set}. One efficient way to construct second order approximation is 

\begin{equation}
\label{tilde_U}
\begin{split}
\tilde{U}(\tau) = \exp(\frac{-i\tau H_{\sigma_\alpha}}{2})\exp(\frac{-i\tau H_{\sigma_x \sigma_x}}{2})\exp(\frac{-i\tau H_{\sigma_y \sigma_y}}{2})\exp(-i\tau H_{\sigma_z \sigma_z})\\\exp(\frac{-i\tau H_{\sigma_y \sigma_y}}{2})\exp(\frac{-i\tau H_{\sigma_x \sigma_x}}{2})\exp(\frac{-i\tau H_{\sigma_\alpha}}{2})
\end{split}
\end{equation}

with !(!should remove t in the braket)

%\begin{equation}
\begin{align}
\label{single_spin}
&\exp(\frac{-i\tau H_{\sigma_\alpha}}{2})= \exp(\frac{-i\tau(-\sum_{i=1}^{N}\sum_{\alpha=x,y,z}h^\alpha_i(t)\sigma^\alpha_i)}{2})\\
\label{double_spin_x}
&\exp(\frac{-i\tau H_{\sigma_x \sigma_x}}{2})=\exp(\frac{-i\tau(-\sum_{i=1}^{N}J^x_{ij}(t)\sigma^x_i\sigma^x_j)}{2})\\
\label{double_spin_y}
&\exp(\frac{-i\tau H_{\sigma_y \sigma_y}}{2})=\exp(\frac{-i\tau(-\sum_{i=1}^{N}J^y_{ij}(t)\sigma^y_i\sigma^y_j)}{2})\\
\label{double_spin_z}
&\exp({-i\tau H_{\sigma_z \sigma_z}})=\exp({-i\tau(-\sum_{i=1}^{N}J^z_{ij}(t)\sigma^z_i\sigma^z_j)})
\end{align}
%\end{equation}

, !where $h^\alpha_i(t), J^x_i(t), J^y_i(t)$, and $J^z_i(t)$ are the strength of Hamiltonian at the current time step according to Equation \ref{Hamiltonian_set}. Equation \ref{single_spin} can be further elaborated as 

\begin{equation}
\label{single_spin 2}
\exp(\frac{-i\tau(-\sum_{i=1}^{N}\sum_{\alpha=x,y,z}h^\alpha_i(t)\sigma^\alpha_i)}{2}) = \prod_{i=1}^{N}\exp(\frac{i\tau \sum_{\alpha_{x,y,z}}h^\alpha_i(t)\sigma^\alpha_i}{2}).
\end{equation}
 
The following equation set out a rotation of the vector $\mathbf{S}$ about the vector $\mathbf{v}$,

\begin{equation}
\label{single_spin 3}
!\exp(i\mathbf{v}\cdot \mathbf{S}) = \mathbb{I} cos(\frac{v}{2}) + \frac{2i\mathbf{v}\cdot \mathbf{S}}{v}sin(\frac{v}{2}),
\end{equation}

where $v$ is the norm of $\mathbf{v}$. With the aid of this equation, the matrix exponential of these single spin terms can be calculated analytically as 

\begin{equation}
\label{single_spin 4}
\exp(i\tau\mathbf{\sigma_i}\cdot\mathbf{h_i}) = \begin{pmatrix}
cos\frac{\tau h_i}{2}+\frac{ih^z_i}{h_i}sin\frac{\tau h_i}{2} & \frac{ij^x_i+h^y_i}{h_i}sin\frac{\tau h_i}{2}\\
\frac{ij^x_i-h^y_i}{h_i}sin\frac{\tau h_i}{2} & cos\frac{\tau h_i}{2}-\frac{ih^z_i}{h_i}sin\frac{\tau h_i}{2}
\end{pmatrix},
\end{equation}

where $h_i$ is the norm of the vector $\mathbf{h_i}=(h^x_i,h^y_i,h^z_i)$. It can be conclude from Equation \ref{single_spin 2}, \ref{single_spin 3}, and \ref{single_spin 4}, that the time evolution for single spin terms turns out to be a $2\times2$ matrix calculation and multiplication. !This can be done by picking right pairs of coefficient in the state vector and applying Equation \ref{single_spin 4}. \\

For the case of double spin operator, since the spin operator with different labels commute, Equation \ref{double_spin_x}, \ref{double_spin_y}, and \ref{double_spin_z} can be decomposed into 

\begin{align}
&\exp(\frac{-i\tau(-\sum_{i=1}^{N}J^x_{ij}(t)\sigma^x_i\sigma^x_j)}{2}) = \prod_{i,j=1}^{N}\exp(\frac{i\tau J^x_{i,j}(t)\sigma^x_i\sigma^x_i}{2})\\
&\exp(\frac{-i\tau(-\sum_{i=1}^{N}J^y_{ij}(t)\sigma^y_i\sigma^y_j)}{2}) = \prod_{i,j=1}^{N}\exp(\frac{i\tau J^y_{i,j}(t)\sigma^y_i\sigma^y_i}{2})\\
&\exp({-i\tau(-\sum_{i=1}^{N}J^z_{ij}(t)\sigma^z_i\sigma^z_j)}) = \prod_{i,j=1}^{N}\exp(i\tau J^z_{i,j}(t)\sigma^z_i\sigma^z_i).
\end{align}

The above expressions, likewise, can be determined analytically and yield 

\begin{align}
\label{double_spin_x 2}
&\exp(\frac{i\tau J^x_{i,j}\sigma^x_i\sigma^x_j}{2}) = \begin{pmatrix}
\cos(J^x_{i,j}\tau) & 0 & 0 & i \sin(J^x_{i,j}\tau) \\
0 & \cos(J^x_{i,j}\tau) & i \sin(J^x_{i,j}\tau) & 0 \\
0 & i \sin(J^x_{i,j}\tau) & \cos(J^x_{i,j}\tau) & 0 \\
i \sin(J^x_{i,j}\tau) & 0 & 0 & \cos(J^x_{i,j}\tau)  
\end{pmatrix} \\
\label{double_spin_y 2}
&\exp(\frac{i\tau J^y_{i,j}\sigma^y_i\sigma^y_j}{2}) = \begin{pmatrix}
\cos(-J^y_{i,j}\tau) & 0 & 0 & i \sin(-J^y_{i,j}\tau) \\
0 & \cos(J^y_{i,j}\tau) & i \sin(J^y_{i,j}\tau) & 0 \\
0 & i \sin(J^y_{i,j}\tau) & \cos(J^y_{i,j}\tau) & 0 \\
i \sin(-J^y_{i,j}\tau) & 0 & 0 & \cos(-J^y_{i,j}\tau)  
\end{pmatrix} \\
\label{double_spin_z 2}
&\exp(i\tau J^z_{i,j}\sigma^z_i\sigma^z_j) = \begin{pmatrix}
\exp(i J^z_{i,j} \tau) & 0 & 0 & 0 \\
0 & \exp(-i J^z_{i,j} \tau) & 0 & 0 \\
0 & 0 & \exp(-i J^z_{i,j} \tau) & 0 \\
0 & 0 & 0 & \exp(i J^z_{i,j} \tau).
\end{pmatrix} 
\end{align}

In a similar way, the time evolution of double spin operators can be evaluated by picking the 4 corresponding states, calculating the matrix, applying the matrix on $\ket{\Psi}$. Moreover, with these analytical expression (i.e. Equation \ref{single_spin 4}, \ref{double_spin_x 2}, \ref{double_spin_y 2}, and \ref{double_spin_z 2} ), it it now possible to carry out $\tilde{U}(\tau)$ in Equation \ref{tilde_U} and apply it to $\ket{\Psi(t)}$. Hence, the time evolution of the wave function can be calculated. \\

With these two algorithm, the full diagonalisation algorithm and the Suzuki-Trotter Product Formula algorithm, it is sufficient to simulate an quantum annealer at zero temperature, namely under ideal condition, on a classical computer. However, in order to simulate a quantum annealer at finite temperature, the computation process will be more complicated and a simple implementation of these two algorithm will not be practical. Further detail will be presented in the next section. \\

%Break the H to small operation that operate on the state. According to $J^x , J^y, and Jz; h^x, h^y, and h^z$, we need to find the pair or quad pair for the $\Psi$


\subsection{Algorithm for system with Temperature Effect}
%Here since we have in total 16 spins, of course we cannot use product formula algorithm. But we because have temperature term now, which means we may run over several states, therefore the product formula algorithm is not enough for use now. Therefore, we use random sampling method.

\subsubsection{Canonical-Thermal State}
%\subsubsection{Boltzmann Distribution/ Assemble Average}

!In order to simulate a system at finite temperature, the quantum subsystem S is coupled to a quantum environment E and the Hamiltonian of the entire system (i.e. S+E) is defined as

\begin{equation}
H = H_S + H_E + GH_{SE},
\end{equation} 

where $H_S$ is the Hamiltonian of the subsystem, $H_E$ is the Hamiltonian of the environment, and $H_{SE}$ is the interaction between the subsystem S and the environment E with $G$ indicates the global coupling strength between S and E. \\

!The quantum subsystem S and the environment E can be described respectively by density matrix 

\begin{equation}
\begin{split}
\rho_S = \ket{S'}\bra{S'}\\
\rho_E = \frac{e^{-\beta H_E}}{Z}, 
\end{split}
\end{equation}

\begin{equation}
\rho(0)=\rho_S\rho_E, \rho(t)=\ket{\Psi(t)}\bra{\Psi(t)}, \ket{\Psi(t)}= U(t)\ket{\Psi(0)}
\end{equation}

\begin{equation}
\begin{split}
&=Tr A(t)\rho(0), ~~A(t)=e^{iHt}Ae^{-iHt}=U^\dagger AU(t)\\
&=Tr A \rho(t), ~~\rho(t)=U^\dagger \rho U(t)\\
&=Tr A(t) \rho_S \rho_E \\
&=Tr A e^{-iHt} \rho_S \rho_E e^{iHt}\\
&=\bra{S}\bra{E}Ae^{-iHt}\ket{S'}\bra{S'}\rho_E e^{iHt}\ket{E}\ket{S}\\
&=\bra{S'}\bra{E} Ae^{-iHt} \rho_E e^{iHt}\ket{E}\ket{S'}\\
&=\sum_{E}\frac{e^{\beta E_E}}{Z}\bra{S'}\bra{E_E} e^{-iHt}Ae^{iHt}\ket{E_E}\ket{S'}\\
&=\sum_{E}\frac{e^{\beta E_E}}{Z}\bra{\Psi(t)}A\ket{\Psi(t)},
\end{split}
\end{equation}

,!!where $S'$ is ???, $\beta$ is the thermodynamic beta defined as $\frac{1}{k_b T}$,  and Z is partition function defined as $Z = \sum_i e^{-\beta E_i}$. In order to calculate the observable numerically, it is convenient to separate the ground state energy, $E^0_E$, from the $\frac{e^\beta E_E}{Z}$. Then $\langle A(t)\rangle$ becomes

\begin{equation}
\langle A(t)\rangle = \sum_{E}\frac{e^{\beta E^0_E}e^{(\beta E_E-E^0_E)}}{e^{\beta E^0_E}Z'}\bra{\Psi(t)}A\ket{\Psi(t)},
\end{equation}

with 

\begin{equation}
\begin{split}
\label{psi_assemble_method}
\ket{\Psi(t)}&=e^{-iHt} \frac{e^{\frac{-\beta H_E}{2}}}{\sqrt{Z}}\ket{\Psi_E}\ket{S'}\\
&=e^{-iHt}\sum_{i} c_i \frac{e^{\frac{-\beta E_E}{2}}}{\sqrt{Z}}\ket{\Psi_E}\ket{S'}
\end{split}
\end{equation}

This is called a canonical-thermal state\cite{Novotny2016}, the Suzuki-Trotter product formula approach is used to calculate the time evolution of this state. The different is that now the same calculation need to be repeated for all different energy eigenstates of the environment. For example, if a subsystem is coupled with a environment which consist of 8 spins, the computation will then need to repeated $2^8$ times for all energy eigenstates. Thus, simply going through all energy eigenstate with Suzuki-Trotter product formula approach becomes not practical because of a long CPU time even for a small environment. A random sampling method that can reduce the CPU time will be introduced in the next subsection.\\

%We still use product formula. and we use assemble average to calculate each initial state exactly.



\subsubsection{Random sampling method}
%\subsubsection{Random wave function}
%Reference check! II Theory part.
%\cite{Hams2000} 

The efficiency of the random sampling method is based on the hypothesis that it can approximate the solution of a time-dependent Sh!!!dinger equation by solving a sample of randomly chosen initial state. According to the central limit theorem, the accuracy of the approximation can be achieved by using a small set of initial states\cite{Hams2000}.  !The trace of a matrix A applying on a D-dimensional Hilbert space spanned by an orthonormal set of states ${\ket{\Psi_n}}$ is given by\\

\begin{equation}
\begin{split}
TrA=\sum_{n=1}^{D} \bra{\Psi_n}A\ket{\Psi_n}\\
\end{split}
\end{equation}

Then a random vector $\ket{\phi}$ can be constructed by choosing D complex random numbers with which mean is 0.

\begin{equation}
\begin{split}
\ket{\phi}= \sum_{n=1}^{D} c_n \ket{{\Psi_n}}, ~~ with ~~c_n \equiv f_n + i g_n\\
\end{split}
\end{equation}

and it follows that

\begin{equation}
\begin{split}
\bra{\phi}A\ket{\phi} = \sum_{m,n=1}^{D} c_m^\star c_n  \bra{\Psi_m}A\ket{\Psi_n}\\
\end{split}
\end{equation}


It is possible to increase the accuracy by generate several samplings. If S realisations are sampled and then averaged out, it yields 

\begin{equation}
\begin{split}
\label{random_sampling}
\frac{1}{S}\sum_{p=1}^{S} \bra{\phi_p}A\ket{\phi_p} = \frac{1}{S} \sum_{p=1}^{S} \sum_{m,n=1}^{D} c_{m,p}^\star c_{n,p}  \bra{\Psi_{m,p}}A\ket{\Psi_{n,p}}
\end{split}
\end{equation}

If there is no correlation between the random numbers in different realisation, and the random number $f_n$ and $g_n$ are drawn from an even and symmetric probability distribution, the following equation can be made.

\begin{equation}
\lim_{S \to \infty} \frac{1}{S} \sum_{p=1}^{S} c_{m,p}^\star c_{n,p} = E(|c|^2)\delta_{m,n},
\end{equation}

where $E(\cdot)$ is the expectation value based on the probability distribution used to draw $c_n$. The subscript $n$ and $p$ of $c_{n,p}$ were removed because it does not depend on $n$ and $p$. Putting this relation back to Equation \ref{random_sampling} leads to 

\begin{equation}
\begin{split}
\lim_{S \to \infty} \frac{1}{S} \sum_{p=1}^{S}  \bra{\phi_p}A\ket{\phi_p} &= E(|c|^2)\sum_{n=1}^{D}\bra{\Psi_n}A\ket{\Psi_n}\\
&=E(|c|^2) TrA
\end{split}
\end{equation}

!It implies that one can compute the trace of A by sampling over random states $\phi_p$ based on the fact that the time evolution can be calculated efficiently by Suzuki-Trotter product formula approach. Namely, random sampling method can provide a statistically reliable result without calculating all energy eigenstates of the environment. !It approximate the answer by sampling over a subset of $D$, the dimension of the original Hilbert space. The statistical error of the random sampling method with a finite large S is $\mathcal{O}(\frac{1}{\sqrt{S}})$, because according the central limit theorem one can write 

\begin{equation}
\frac{1}{S} \sum_{p=1}^{S} c_{m,p}^\star c_{n,p} = E(|c|^2)\delta_{m,n} + \mathcal{O}(\frac{1}{\sqrt{S}})
\end{equation}



\newpage
\section{Quantum Annealer Simulation: Ideal Case}
\label{result_ideal}

For the ideal case, the system consists of 8 spins in the simulation. There is no heat bath coupled with the system, which means there is no temperature effect and the system is in its ideal condition. The time scheme of the annealing is linear time scheme.  \\

%	There is no ideal system. Environment will always affect the subsystem. However, I start from a simple case, that is a subsystem without environment effect. 
%\subsection{Simulation Set up}

\subsection{Result}
\subsection{the evolution of the spin, energy, and success probability during the annealing}

%	\begin{itemize}
%		\item \checkmark Figure here: The system energy vs. lambda
%		\item \checkmark Figure here: The spin x vs. lambda
%		\item \checkmark Figure here: The spin z vs. lambda
%		\item \checkmark Figure here: The success probability vs. lambda
%	\end{itemize}

For the ideal case, the simulation methods used are the full diagonalisation method and suzuki-trotter product formula approach. First, the result from the suzuki-trotter product formula approach is compared with the results from the full diagonalisation method in Figure \ref{compare_fd_pf}, in order to validate the correctness of the result from the suzuki-trotter product formula approach. At the beginning of the annealing, the system is in the ground state of the initial Hamiltonian, so the success probability is low. During the annealing, the system slowly transit from the ground state of the initial Hamiltonian to the ground state of the problem Hamiltonian. As a result, the success probability proceeds towards one at the end of the annealing process. \\

\begin{figure}
	\centering
	
	\begin{subfigure}[t]{0.55\textheight}
%		\centering
		\includegraphics[width=\linewidth]{Figure_compare_1e0.eps}
		\caption{$\tau$ = 1.0}\label{fig:fig_a}
	\end{subfigure}
	\\
	\begin{subfigure}[t]{0.55\textheight}
%		\centering
		\includegraphics[width=\linewidth]{Figure_compare_1e-1.eps}
		\caption{$\tau$ = 0.1}\label{fig:fig_b}
	\end{subfigure}
	\\
	
	\begin{subfigure}[t]{0.55\textheight}
%		\centering
		\vspace{0pt}% set the real top as the top
		\includegraphics[width=\linewidth]{Figure_compare_1e-2.eps}
		\caption{$\tau$ = 0.01}\label{fig:fig_c}
	\end{subfigure}
	\\
	\begin{minipage}[t]{\linewidth}
%		\centering
		\caption{ A comparison of the result from the Suzuki-Trotter product formula approach (blue curve) and the result from the full diaganolisaiton method (red curve) for the success probability as a function of the annealing progress $t / T$ for different time steps,  $\tau$. The success probability is defined as the overlap of the current wave function with the wave function of the exact ground state, $\bra{\Psi(t/T)}\ket{\Psi_{gs}}$, in which the exact ground state is known beforehand. }	
		\label{compare_fd_pf}
	\end{minipage}
\end{figure}

The energy of the system is also an important observable to watch on during the annealing. The result of a particular 2-SAT problem is shown in Figure \ref{fig:energy_evo}. The system starts with an energy equal to $-8$ because its in the ground state of $H_{init}$, which is $-\sum_{i=1}^{8}h_i^x \sigma_i^x$ for a 8 spins system. At the end of the annealing, the system becomes in the ground state of $H_{problem}$, which is $-\sum_{i=1}^8 h_i^z \sigma^z_i - \sum_{i,j =1}^8 J_{ij}^z \sigma^z_i \sigma^z_j$. In this particular case, the ground state energy of $H_{problem}$ is $-9$. \\

\begin{figure}
	\centering
%	\includegraphics[width=\linewidth]{result_picture/energy_evolution_product_formula/with_product_formula/Figure_productf_EnergyvsLambda.eps}
	\includegraphics[width=\linewidth]{Figure_productf_EnergyvsLambda.eps}
	\caption{The energy evolution of an 8 spins system for a particular 2-SAT problem.}
	\label{fig:energy_evo}
\end{figure} 

Another interesting observable is the spin value which is shown in Figure \ref{fig:spin_evo}. In similar fashion, since the system starts from the ground state of $H_{init}$, the value of $\sigma_1^x, \sigma_2^x \cdots \sigma_8^x$ are $1$. During the annealing, these values approach $0$. On the other hand, the value of $\sigma_1^z, \sigma_2^z \cdots \sigma_8^z$ evolves from $0$ towards the configuration of the ground state of $H_{problem}$.\\

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{result_picture/probability_spin_system/Figure_Annealing_sigma_value.eps}
	\caption{The evolution of the spin values for a particular 2-SAT problem. }
	\label{fig:spin_evo}
\end{figure}

To be brief, the simulation result indicates that a successful quantum annealing process can leads the system to the ground state of problem Hamiltonian and let the spin value end up in the ground state configuration. According to the adiabatic theorem and the Landau-Zener formula, the total annealing time and the minimun gap between the ground state and the first excited state may effect the quantum annealing process. The result of these effect will be shown in the following subsection. \\
%	A general picture on the annealing behaviour. The expectation value of spin should start from x direction and end up in z direction. During the process y may only have some fluctuation. For the energy of subsystem we expect it to first increase and then decrease to a lower level. We knew the ground state ready, so in the end we expect to see the ground state evolute form 0 to 1, if this is a successful annealing.

%\subsubsection{The Effect of step size tau}
%	After having a look at the general properties. Now I try to investigate the size of time step. When the Hamiltonian is time-dependent. The tau should be small enough.  other wise the result would deviate. 
%	\begin{itemize}
%		\item \checkmark Figure here: compare the result of product formula with the result of full diagonalization: success probability vs. annealing Time with different tau.
%	\end{itemize}
%%	\begin{figure}[h!]
%%		\centering
%%		\includegraphics[height=300pt]{result_picture/compare_full_product/Figure_product_formula.png}
%%		\caption{Insert caption here.}
%%		\label{example_figure}
%%	\end{figure}
%%	\begin{figure}[h!]
%%		\centering
%%		\includegraphics[height=300pt]{result_picture/compare_full_product/Figure_full_diag.png}
%%		\caption{Insert caption here.}
%%		\label{example_figure}
%%	\end{figure}


\subsubsection{The Effect of Annealing Time}
%	Setting up the proper $\tau$. Then I start to simulate the annealing process with different annealing time. The annealing should be long enough to find a ground state. Also I compare the result of full diagonalization and product formula algorithm. 
%	\begin{itemize}
%		\item Figure here: compare the result of different annealing time of full diagonalization: success probability vs.lambda with different annealing time
%		\item \checkmark Figure here: compare the result of different annealing time of full diagonalization: success probability vs.lambda with different annealing time
%		\item \checkmark Figure here: compare the result of different annealing time of full diagonalization: energy vs.lambda with different annealing time
%		\item \checkmark Figure here: compare the result of different annealing time of full diagonalization: energy vs.lambda with different annealing time
%	\end{itemize}

The adiabatic theorem requires a slowly varying Hamiltonian to make the system remain in the ground state while its eigenenergy evolves continuously. In other words, the total annealing time should be long enough. The effect of the total annealing time can be seem in Figure \ref{fig:effect_time_probability}. When the total annealing time is not long enough, the success probability can not reach one in the end. Instead of sticking to the ground state energy, the annealing path of the energy enters a mixed state of the ground state and the other excited state, which ends in a higher energy state. This effect is shown in Figure \ref{fig:effect_time_energy}.  \\

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{Figure_Annealing_probability.eps}
	\caption{The evolution of success probability with different total annealing time.}
	\label{fig:effect_time_probability}
\end{figure}


\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{Figure_productf_EnergyvsLambda_time.eps}
	\caption{The evolution of energy with different total annealing time.}
	\label{fig:effect_time_energy}
\end{figure}


\subsubsection{The Effect of Minimum Gap}
%	According to Landau-Zener theorem, the successful probability depend on the gap in between the ground state and the first excited state.
%	\begin{itemize}
%		\item \checkmark Figure here: energy spectrum: the energy for all states vs. lambda
%		\item \checkmark Figure here: a close view of the result above: the energy for lowest 20~30 states vs. lambda
%		\item \checkmark Figure here: compare: the success probability under the annealing time that maximise the minimum gap difference vs. minimum gap value 
%	\end{itemize}
%	\begin{figure}[h!]
%		\centering
%		\includegraphics[height=300pt]{result_picture/energy_evolution_product_formula/with_product_formula/Figure_energy_spectrum.png}
%		\caption{Insert caption here.}
%		\label{example_figure}
%	\end{figure}
%	
It is known that the minimum gap will effect the annealing according to the Landau-Zener formula. With Eq. \ref{eq:landau}, the success probability is expected to decrease when the minimum gap decreases. In Figure \ref{fig:energy_spectrum_whole}, an example energy spectrum of a particular 2-SAT problem is shown. The minimum gap is always defined as the smallest gap between the ground state and the first excited state through out this work. \\

\begin{figure}
	\centering
	
	\begin{subfigure}[t]{0.55\textheight}
	\centering
	\includegraphics[width=\linewidth]{Figure_energy_spectrum.eps}	
	\caption{}
	\label{fig:energy_spectrum}
	\end{subfigure}
	\\
	\begin{subfigure}[t]{0.55\textheight}
		\centering
		\includegraphics[width=\linewidth]{Figure_closelook_energy_spectrum.eps}	
		\caption{}
		\label{fig:energy_spectrum_close}
	\end{subfigure}
	\\
	\begin{minipage}[t]{\linewidth}
		%		\centering
		\caption{ (a) The energy spectrum of a particular 2-SAT problem. The minimum gap between the ground state and the first excited state occures at $t/T = 0.475$ with value$=0.393014$. (b) A close look on the energy spectrum. }	
		\label{fig:energy_spectrum_whole}
	\end{minipage}
\end{figure}

In Figure \ref{fig:minimum_gap_ideal}, each cross point stands for the simulation result of a particular 2-SAT problem and in total 100 2-SAT problems are shown in the plot. The trend between the success probability and the minimum gap of the simulation does agree with what the Landau-Zener formula describes. Base on this result, it can be seen that the difficulty of a problem rely on the value of the minimum gap. If the minimum gap of a 2-SAT problem is quite small, the system may need longer total annealing time to reach the ground state of the problem Hamiltonian.  \\

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{Figure_PvsGap_no_coupling_Temp1.eps}	
	\caption{A plot with 100 different 2-SAT problems. The total annealing time of the simulation is 200.	 The green line is the fit to $P_{adiabatic} = 1-\exp(-c\cdot \Delta_{min}^2)$. The asymptotic standard error is 0.0178.}
	\label{fig:minimum_gap_ideal}
\end{figure}



\newpage
\section{Quantum Annealer Simulation with Temperature Effect}
\label{result_temp}
%After the case without environment, now we move on to the one with environment.

In the previous section, the simulation result of the ideal case has been presented. However, in the real world, the system will always be influenced by the environment. The main difference is the temperature. For the ideal case, the system is perfectly isolated from the environment and stay at zero temperature. On the other hand, for the real case, the system evolves with finite temperature instead of zero temperature. \\
	
In order to simulate the influence of the finite temperature, the quantum system S is coupled to a heat bath B and the Hamiltonian of the entire system (i.e. S+B) is defined as
	
\begin{equation*}
	H = H_S + H_B + gH_{SB},
\end{equation*} 
	
where $H_S$ is the Hamiltonian of the system, $H_B$ is the Hamiltonian of the heat bath, and $H_{SB}$ is the interaction between the subsystem S and the heat bath B with $g$ indicates the global coupling strength between S and B. \\
	
	
%\subsection{Simulation Set up}
%	Here the system consist of a 8-spins subsystem and 8-spins environment. The interaction depend on the coupling factor. When factor is 0, the subsystem cannot be affected by the environment at all. On the other, if the factor is 1, the spin of environment may fully interact with subsystem just as one of the spin inside.

\subsection{Result}

\subsubsection{Validation of the Random Sampling Method}

	In principal, the simulation of the entire system can be done with the Suzuki-Trotter product formula approach and approximate the heat bath by canonical thermal state. However, when the size of the system increase, the computational time become not practical. Consequently, the random sampling method is much preferred in order to reduce the computational time. A result comparison of the random sampling method and the canonical ensemble approach is shown in Figure \ref{fig:validation_random_sampling}. The size of the system and the heat bath both are 8 spins. Different colour states for different coupling strength. It is obvious that under this situation, the result is not stable with only one run. If the simulation is repeat several times with different random wave function, the average result can be close to that of the canonical ensemble approach.  \\
	
\begin{figure}
	\centering
	
	\begin{subfigure}[t]{.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Figure_JvsT_T1e0_run1.eps}	
		\caption{}
%		\label{fig:energy_spectrum}
	\end{subfigure}
	\begin{subfigure}[t]{.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Figure_JvsT_T1e0_run2.eps}	
		\caption{}
%		\label{fig:energy_spectrum_close}
	\end{subfigure}

	\begin{subfigure}[t]{.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Figure_JvsT_T1e0_run3.eps}	
		\caption{}
		%		\label{fig:energy_spectrum_close}
	\end{subfigure}
	\begin{subfigure}[t]{.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Figure_JvsT_T1e0_run4.eps}	
		\caption{}
		%		\label{fig:energy_spectrum_close}
	\end{subfigure}
	
	\begin{subfigure}[t]{.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Figure_JvsT_T1e0_random.eps}	
		\caption{}
		%		\label{fig:energy_spectrum_close}
	\end{subfigure}
	\begin{subfigure}[t]{.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Figure_JvsT_T1e0.eps}	
		\caption{}
		%		\label{fig:energy_spectrum_close}
	\end{subfigure}

	
	\begin{minipage}[t]{\textwidth}
		\centering
		\caption{Each colour states for a different coupling strength between the system and the heat bath. Along the same colour line, each data point represent an annealing result with different total annealing time. The temperature of this set of simulation is 1. Plot (a), (b), (c), and (d) are the results of a single run correspondingly with the random sampling method. Plot (e) is the average result of the 12 runs. Plot (f) is the result computed by going over all canonical ensemble state. It can be seen that Plot (e) and Plot (f) are similar to each other.}	
		\label{fig:validation_random_sampling}
	\end{minipage}
\end{figure}


\subsubsection{The Effect of Temperature}

A set of simulation result with different temperature is shown in Figure \ref{fig:diff_temperature_heat_bath}. Base on this, some interpretations can be made. First, the purple line always stays the same across different temperature simulation. The reason for this is the zero coupling factor between the system and the heat bath. That is, although the temperature of the heat bath changes, it cannot influence the system. Hence the system is isolated from the environment as it is in the ideal case. Second, the effect of finite temperature does harm the annealing process. However, the effect of this does not uniformly reduce the success probability. Instead, the overall success probability does decrease, but with the trend of going up when the total annealing time is short, then going down, and going up again when the total annealing time is longer. A further detail of this effect is shown in Figure \ref{fig:coherent-non-quasi}. It can be explained by the thermal equilibrium process between the system and the heat bath\cite{Amin2015}. When the total annealing time is short, the heat bath does not yet have enough time to influence the system and the system can stay in its coherent state. In this case, the way that the system evolves during the annealing is similar to the case in ideal condition. As a result, the success probability increase when the total annealing time gets longer. After the total annealing time increase to a certain range, it becomes enough for the heat bath to influence the system. Inside this regime, the heat bath start to really interact with the system and harm the coherency of the system. The entire system enters a non-equilibrium state. Therefore, when the total annealing time increase, the success probability goes down. If the total annealing gets even longer, the system and the heat bath will reach a quasi-static state. That is the influence from the heat bath happens slowly enough for the system to keep itself in an equilibrium state. Therefore, the system can again evolves likes it is in an ideal condition and the success probability can become larger with longer total annealing time. \\
	
\begin{figure}
	\centering
	
	\begin{subfigure}[t]{.55\textheight}
		\centering
		\includegraphics[width=\linewidth]{result_picture/JvsT_plot/Exact/Figure_JvsT_T2e-2.eps}	
		\caption{A result at Temperature = 0.02}
		%		\label{fig:energy_spectrum}
	\end{subfigure}
	
	\begin{subfigure}[t]{.55\textheight}
		\centering
		\includegraphics[width=\linewidth]{result_picture/JvsT_plot/Exact/Figure_JvsT_T1e0.eps}	
		\caption{A result at Temperature = 1}
		%		\label{fig:energy_spectrum_close}
	\end{subfigure}
	
	\begin{subfigure}[t]{.55\textheight}
		\centering
		\includegraphics[width=\linewidth]{result_picture/JvsT_plot/Exact/Figure_JvsT_T1e3.eps}	
		\caption{A result at Temperature = 1000}
		%		\label{fig:energy_spectrum_close}
	\end{subfigure}
	
	\begin{minipage}[t]{\textwidth}
		\centering
		\caption{The simulation is done with canonical ensemble approach.}	
		\label{fig:diff_temperature_heat_bath}
	\end{minipage}
\end{figure}

\begin{figure}
	\centering
	
	\begin{subfigure}[t]{.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Figure_JvsT_T2e-2_coherent.eps}	
		\caption{Temperature = 0.02}
		%		\label{fig:energy_spectrum}
	\end{subfigure}
	\begin{subfigure}[t]{.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Figure_JvsT_T1e0_coherent.eps}	
		\caption{Temperature = 1}
		%		\label{fig:energy_spectrum}
	\end{subfigure}

	
	\begin{subfigure}[t]{.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Figure_JvsT_T1e3_coherent.eps}	
		\caption{Temperature = 1000}
		%		\label{fig:energy_spectrum_close}
	\end{subfigure}
	\begin{subfigure}[t]{.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Coherent.png}	
		\caption{}
		%		\label{fig:energy_spectrum_close}
	\end{subfigure}
	
	\begin{minipage}[t]{\textwidth}
		\centering
		\caption{In Figure (a) the temperature is so low that the heat bath is also in its ground state, so the temperature effect does not influence the system much and both curve are quite similar. On the other hand, in Figure (c) the temperature is too high and the heat bath damage the coherence of the system a lot within the non-equilibrium regime. Consequently, the success probability becomes very low and very hard to go up again. In Figure (b) the heat bath is in a moderate temperature which can show the phenomenon more clearly. The curve first goes up because the system is still in its coherent state. Then the curve goes down in the cause of the influence of the heat bath. The curve goes up again since the entire system enters a quasi-static state and the system can evolves coherently.}	
		\label{fig:coherent-non-quasi}
	\end{minipage}
\end{figure}

%\subsubsection{The Process toward Quasi-static}
%	See Fig 3 of this reference	\cite{Amin2015} 
%	
%	In the beginning of the annealing. The probability of finding the ground state may still increase, because the environment haven't yet to affect the subsystem. Then when the environment start interacting with subsystem. After it equilibrium with the subsystem. It will continuous to anneal and the probability will increase. 
%	\begin{itemize}
%		\item \checkmark Figure here: display plots with different temperature: success probability with different coupling factor vs. annealing time
%		
%		\item \checkmark Figure here: repeat above plots with Random wave function
%	\end{itemize}
%%
%%	\begin{figure}[h]
%%		\centering
%%		\includegraphics[height=300pt]{result_picture/JvsT_plot/T1/JvsTwTemp1.png}
%%		\caption{Insert caption here.}
%%		\label{example_figure}
%%	\end{figure}
%	
%\subsubsection{The Effect of Annealing Time}
%	This subsection may be removed 


\subsubsection{The Effect of the Minimum Gap}

It is already known from the Landau-Zener formula that the minimum gap has an effect on the difficulty of the annealing process. If the minimum gap is small, the annealing may need longer annealing time to keep on the ground state instead of a mix state with higher energy during the process. In the ideal case, the simulation result can be fitted well with the model as shown in Figure \ref{fig:minimum_gap_ideal}. Following the similar set up, the simulation is done again but with the heat bath. Since each instance is a 16 spins problem (i.e. S = 8, B = 8) and in total 100 2-SAT problems are involved, the algorithm used here is the random sampling method in order to reduce the computational time. The result is shown in Figure \ref{fig:minimum_gap_coupling} and Figure \ref{fig:minimum_gap_temperature}.\\
	
In Figure \ref{fig:minimum_gap_coupling}, the plot with $g=0.0$ can be considered as the result in the ideal condition, because there is no interaction between the system and the heat bath. If the coupling factor is turned on, the first effect is the drop of the success probability. It can be seen clearly that when the success probability of the result with $g = 0.0$ climbs up to around $0.8$, the success probability of the result with $g = 1.0$ stays only between $0$ and $0.2$. Furthermore, the role of the coupling factor not only cut down the success probability, but also causes scattering. In the result with $g = 0.0$, the data points can almost form a smooth line, which can be describe by the Landau-Zener formula. On the other hand, in the result with $g = 0.5$ the data point scatter in the range of $0$ to $0.5$. However, in the result with $g = 0.0$ the range of scattering become narrower. This can be understood as a combined effect of the both. The strong interaction between the system and the heat bath suppresses the growth of the success probability even with a greater minimum gap value. Although a large coupling factor tend to enlarge the range of scattering, this range is made narrower because of the reduction of the overall success probability. As a result, the range of scattering in the result $g = 1.0$ is narrower than that in the result with $g = 0.5$.\\
	
In Figure \ref{fig:minimum_gap_temperature}, the similar fact can be observed. In the result with temperature $= 0.01$, it can be seen that with an enough annealing time, the success probability can go up to around $1$. The increase of the temperature causes the drop of the overall success probability and the scattering of the data point. The comparison of the result with temperature $= 0.5$ and the result with temperature $= 1.0$ also shown that a combined effect of the both may narrow the range of the scattering instead of enlarging it. It is worth explaining why there is no much difference between the result with temperature $= 0.01$ and the result with temperature $= 0.05$. The reason is that under this particular simulation setting the heat bath is in its ground state with both temperature. Consequently, the result of these two setting are quite similar to each other.\\
	
%	the scatter effect come from the spin numbering. When the coupling factor become larger and larger. The sccatering effect become more clear. On the other hand, Temperature is not the critical reason to this phenomenon. But the temperature will also enlarge the scatter. The temperature has an influence on how many environment state are part of the evolution. If the temperature is low the revolution only in the ground state. When temperatrue is high, almost all environment state is part of the evolution. the state is average out by the environment states. So the scatter of the problem hamiltonian is not clear. Also since we are in an effective temperature unit. delta gap/Temperature. Therefore, when temperature is larger than gap, they are in excited state. so the annealing is not clear and cant find the ground state. vice versa. when the temperature is smaller than 1. we can see the annealing effect and probability to find the ground state.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{Figure_PvsGap_T2e2_Temp1.eps}	
	\caption{A set of simulation result with 100 different 2-SAT problems. The total annealing time of the simulation is $200$ and the temperature is $1$.}
	\label{fig:minimum_gap_coupling}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{Figure_PvsGap_T2e2_Temp1_avoid_duplicate.eps}	
	\caption{A set of simulation result with 48 2-SAT problems with different minimum gap values. The total annealing time of the simulation is $200$ and the temperature is $1$.}
	\label{fig:minimum_gap_coupling_avoid_duplicate}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{Figure_PvsGap_T1e3_G02.eps}	
	\caption{A set of simulation result with 100 different 2-SAT problems. The total annealing time of the simulation is $1000$ and the coupling factor is $0.2$.}
	\label{fig:minimum_gap_temperature}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{Figure_PvsGap_T1e3_G02_avoid_duplicate.eps}	
	\caption{A set of simulation result with 48 2-SAT problems with different minimum gap values. The total annealing time of the simulation is $1000$ and the coupling factor is $0.2$.}
	\label{fig:minimum_gap_temperature_avoid_duplicate}
\end{figure}

%\section{??D-wave Practice}
%	\cite{Johnson2011}
%d-wave use of super conducting qubit
%	
%\section{??Parallelization}
%	\definecolor{light-gray}{gray}{0.4}
%	\textcolor{light-gray}{Shoul I put the result of openMP parallerization?}
%	It is obvious that i cant put the parallization in between the different time step, because they depend on each other. So i put openmp in side each step for the state. also because the calculation may cause race condition, i have to put it in the second loop instead of the most outside one. this may decrease the optimisation of the running time. However, i think this is the moderate way to get a balance of not getting code too complicate and too slow running time.
%	
%	! \textcolor{red}{The runtime difference for a single run with core 1,2,4,8,16,32,64,128.}




\newpage
\section{Applicatoin on Machine Learning / Simulation}
	
The properties of the quantum annealing in dealing with optimisation problem makes it a potential candidate for machine learning application. A tree cover detector based on aerial photography was implemented with quantum annealer processor \cite{Boyda2017}. Boosting is the learning algorithm used in the paper. The main idea of boosting is that a strong classifier can be constructed by properly combining a set of weak classifier. A weak classifier is required to classify only slightly better than random guess, which is $50\%$. The critical part of boosting is to find a proper weight of these weak classifiers and build the strong classifier. In order to fit the algorithm with a quantum annealer, the original boosting formula can be modified into the following one [??]. \\
	
\begin{equation}
	C(t) = sign\left(\sum_{i=1}^{N} w_i c_i(t)\right)
\end{equation} 
	
, where t is the data sample, $c_i\in\{-1,+1\}$ is the weak classifier, and $w_i\in\{0,+1\}$ is the weight of the weak classifier. The weights are required to be binary in this modified version. As a result, the strong classifier becomes just a majority vote of the weak classifiers. The cost function choose here is regulated quadratic loss function. With a set of training data, $T$, if each single data $t$ has been classified correctly and labelled by $y(t)\in\{-1,+1\}$, the goal of the learning is \\
	
\begin{equation}
	\operatorname*{arg\,min}_{w_i} \left\{ \sum_{t\in T} \left( \sum_{i=1}^{N} w_i c_i(t) - y(t) \right)^2 + \lambda \sum_{i=1}^{N} w_i  \right\}
\end{equation}
	
, where $\lambda$ is the regularisation parameter. The formula implies that if a weak classifier, $c_i(t)$, can in general give data the same label as $y(t)$ does, it is better to keep this weak classifier, which can be done by setting $w_i$ to $1$. On the other hand, if $c_i(t)$ cannot labelled the training data set correctly most of the time, this classifier should probably be discarded. The role of regularisation parameter is to prevent overfitting. That is, only the important weak classifier are selected as part of the strong classifier. If the algorithm tries to include more weak classifiers than what it actually needs, the regularisation parameter will penalise this behave by raising up the cost value. This formula can be rewritten as following by expanding quadratic part.\\
	
\begin{equation}
	\operatorname*{arg\,min}_{w_i} \left\{
	\sum_{i}\left( \lambda -2\sum_{t\in T}c_i(t)y(t) \right) w_i + \sum_{i,j}\left( \sum_{t\in T} c_i(t)c_j(t) \right) w_i w_j +const
	\right\}
\end{equation}
	
The format of the argument is similar to the optimisation problem that can be solved by a quantum annealer. However, the quantum annealing algorithm prefers variables $w_i$ to be in a set of $\{-1,+1\}$ instead of $\{0,+1\}$. A transformation can be made by assigning $s_i = 2w_i -1$ and the formula becomes \\
	
\begin{equation}
	\operatorname*{arg\,min}_{s_i} \left\{
	\sum_{i}\left( \frac{\lambda}{2} -\sum_{t\in T}c_i(t)y(t) +\frac{1}{2}\sum_{j}\left( \sum_{t\in T} c_i(t)c_j(t) \right)\right) s_i + \frac{1}{2}\sum_{i>j}\left( \sum_{t\in T} c_i(t)c_j(t) \right) s_i s_j +const
	\right\}
\end{equation}
	
With this proper format, the quantum annealer can step in and solve the optimisation problem. Another possible way to construct this format is simply setting $w_i$ in a set of $\{-1,+1\}$, but there is no corresponding regularisation part to prevent the algorithm from overfitting. The performance of these two formats depends on the dataset and needs to be tested out. \\
	
The second application is a feature extraction of the human face implemented on a quantum annealer\cite{OMalley2017}. The use of a feature extraction is to transform a large set of raw data into a reduced set of informative and non-redundant feature. In order to implement this with quantum annealer processor, nonnegative binary matrix factorisation (NBMF) is used. NBMF can be described by the following factorisation. \\
	
\begin{equation}
	V^{n\times m} \approx W^{n\times k}H^{k\times m}
\end{equation}
	
,where n is the dimension of the data, m is the number of the data, k is the number of the feature extracted and $k < n$. The matrix elements of $W$ must be nonnegative and the matrix elements of $H$ must be binary. The goal of the learning is \\
	
\begin{equation}
	\operatorname*{arg\,min}_{H}  \| V-WH \|_F 
	\label{eq:NBMF}
\end{equation}
	
,where $\|\cdot\|_F$ is the Frobenius norm. This formula can be decomposed into a set of independent optimisation problems based on each column of $H$, because $i^{th}$ column of $WH$ only depends only on the $i^{th}$ column of $H$ and other columns of $H$ do not have any impact on this at all. Therefore the Equation \ref{eq:NBMF} can be simplified a series of the following equation for $i = 1, 2, \cdots, m$ . \\
	 
\begin{equation}
	H_i = \operatorname*{arg\,min}_{q}  \| V_i - Wq\|_2 
\end{equation} 
	 
,where $H_i$ denotes the $i^{th}$ column of $H$ and $V_i$ denotes the $i^{th}$ column of $V$. In order to solve this linear least squares problem with a quantum annealer, a further transformation can be made and it results in \\
	
\begin{equation}
	\operatorname*{arg\,min}_{q}\left\{  
	\sum_{i}^{k}\left( \sum_{r}^{n} V_r W_{ir} \right) q_i + \sum_{i,j}^{k} \left( \sum_{r}^{n}W^T_{ir}W_{rj}\right) q_i q_j + const
	\right\}
\end{equation}
	 
Although with this form $q_i$ will be solved in a set of $\{-1,+1\}$ by a quantum annealer rather than the required binary set $\{0,+1\}$, the feature extraction can still be achieved. One can reform the formula by substituting variable $s_i=2q_i -1$ as the previous example and make the final configuration of $q_i$ in $\{-1,+1\}$. The performance of these both again can only be benchmarked with real data.\\
	 
As the two example shown above, the quantum annealing technique can be considered as a different computational approach when implementing some machine learning algorithms. However, there are some drawbacks. First, the original algorithm needs to be transformed into a combinatorial optimisation problem that can be solved by a quantum annealer. This may constrain the generality of the quantum annealing technique, because not all machine learning algorithm can be transformed into a combinatorial optimisation problem. The second drawback is the limitation of the problem size. The computational power offered by the currently available machine restricts the size to be quite small when comparing to what a conventional computer can solve nowadays. These imperfect properties nonetheless make the quantum annealing suitable to certain kind of machine learning that needs large amount of simple computations without requiring a high accuracy, such as boosting. To summarise, while the machine learning has been identified as one of the area where the quantum annealing technique may be useful, whether it will become a competitive player in the long run depends on the maturation of its hardware, which is the quantum annealer.  \\	 
	
%	\cite{Adachi2015}
%	\cite{Benedetti2016}
%	\cite{Boyda2017}
%	\cite{OMalley2017}
%	\cite{Potok2017}
%	



\newpage	
\section{Conclusion}

This work attempted to simulate a quantum annealer solving the 2-SAT problem at zero and finite temperature. After encoding the 2-SAT problem into the problem Hamiltonian, the simulation can be done by solving a time-dependent Schrödinger equation which describes the quantum annealing process. \\


For the zero temperature case, the first algorithm used is the full diagonalisation. It is a straightforward approach and standard libraries, such as LAPACK, are available for diagonalising matrix. The limitation is that the size of the problem cannot be large, otherwise the computational time becomes not practical. In spite of this, the full diagonalisation method can well serve as a validation for other algorithm when solving the time-dependent Schrödinger equation. The second algorithm used is the Suzuki-Trotter product formula approach. With this approach, one can decompose a matrix exponential into a series product of small matrix exponential. If the analytical expression of the small matrix exponential is known, the simulation can be done without matrix diagonalisation. This approach is validated by the full diagonalisation method and then be used to simulate a 8 spin system at zero temperature. \\

For the finite temperature case, the temperature effect is provided by coupling a heat bath to the system. The heat bath can be approximated by the canonical ensemble. The quantum annealing process of the entire system can then be simulated by running simulation on the system and every energy state of the hear bath respectively. The computational time of this approach grows linearly with the number of the energy state of the hear bath. Therefore, if the size of the heat bath is large, this approach cannot be considered practical. To reduce the computational time in a proper range, the random sampling method is presented. The main assumption is that the heat bath can be approximate with a random initial state, which means the simulation can be done with this random initial state instead of all energy states. Although multiple runs are usually necessary in order to improve the accuracy, the random sampling method still reduces the computational time greatly. \\

The simulation result of the quantum annealing at zero temperature matched what Landau-Zener formula describes. the success probability decreases, when the minimum gap value gets smaller. This also indicates that the difficulty of a quantum annealing process can be estimated by its minimum gap value. The result at zero temperature also shown that without enough annealing time, the system cannot end in the ground state of the problem Hamiltonian, which is described by the adiabatic theorem. \\

The simulation result of the quantum annealing at finite temperature shows that the trend of the success probability can be explained by thermal equilibrium. When the total annealing time is short, the heat bath has not yet influenced the system and the system can evolve coherently. If the total annealing time becomes longer, the heat bath and the system start to really interact with each other. The success probability reduces with longer total annealing time in this regime. If the total annealing time gets ever longer, the entire system reaches a quasi-static state. As a result the system can again evolve coherently and the success probability increases with longer total annealing time. The effect of the temperature and the coupling factor lowers the overall success probability. Furthermore, the effect of these both causes data to be scattering instead of a smooth line described by the Landau-Zener formula. \\

In summary, the quantum annealing technique does provide a different approach to solve the 2-SAT problem. Furthermore, this technique can be applied in other area such as machine learning, if one can transform the problem into a proper form for a quantum annealer. While a practical use of the quantum annealer relies on the future maturation of the hardware, this work demonstrated some general properties of a quantum annealer when solving an optimisation problem and all simulation done in this work can be implemented in a quantum annealer hardware for comparison.\\


 
%The quantum annealing process of the entire system can be simulated by approximating the heat bath with canonical ensemble and running the Suzuki-Trotter product formula approach on the system with every energy state of the heat bath. 




 







%\section{miscellaneous}
%
%Please specify your name, matriculation number, name of advisor and the title of your report in \linebreak
%\verb+titlepage.tex+.
%The title page will not count for the 20 pages.
%
%Using bibtex you can cite in an organized way and without much work.
%Just enter the information about a paper or an article you want to cite in the \verb+seminar_report.bib+ file and use \verb+\cite+ to cite them. For example \cite{Author08CVPR},\cite{Author04IJCV}.
%Don't forget to compile the bib file and Latex will add all the cited references at the end.
%Cite all the literature you use and state where figures are from!
%
%I am a section. Latex will give me a number \emph{automatically} and put me into the table of contents.
%Using \verb+\label+ and \verb+\ref+ you can use Latex to write that this is Section \ref{section}.
%
%
%
%\subsection{a subsection}
%I am a subsection.
%
%\begin{itemize}
%	\item I am an item.
%	\item [-] I am another item.
%\end{itemize}
%
%\subsubsection{a small subsection}
%I am a subsubsection, an even smaller subsection.
%
%\begin{tabular}{|l|c|}
%\hline
%I am a tabular & with two columns. \\
%\hline
%The left column is aligned left & and the right columns is centered. \\
%\hline
%\end{tabular}
%
%
%\begin{figure}[h]
%\centering
%\includegraphics[height=100pt]{doge.jpeg}
%\caption{Insert caption here. Image from \cite{lenna}. }
%\label{example_figure}
%\end{figure}
%Figure \ref{example_figure} also gets a number automatically and will be placed where Latex thinks it looks good. You can specify a preference with h(ere), t(op), b(ottom), p(age).
%
%
%%\begin{figure}
%%%	\begin{center}
%%		\resizebox{!}{!}{\input{pvsgap}}
%%%	\end{center}
%%\end{figure}
%
%Latex is also really good at printing equations: $E=mc^2$
%
%\begin{equation}
%A = \sum_{i=1}^N A_1 \cdot A_2
%\end{equation}
%
%\subsubsection{Possible Material}
%Result on spin and Energy env,sys,se
%
%success probability of ground state
%
%spin system
%
%energy spectrum
%
%evolution of spin and energy
%
%Runtime comparison
%
%Parallel possibility
%
%-Introduction
%
%-Annealing Theorem
%
%-Full-diagonalization
%
%-suzuki-trotter product formula
%
%-size of tau
%
%-The effect of annealing Time w/ w/o Heat bath
%
%-The effect of minimum gap w/ w/o Heat bath
%
%-Landau-Zener Theorem
%
%-2-SAT problem 
%
%-Random wave function  
%
%-The effect of heat bath(Coherent - Transverse - quasiequilibrium) at different temperature
%
%D-wave practice
%
%-Hamiltonian Mapping
%
%-Annealing Time scheme
%
%-Time-Dependent Schrï¿½dinger Equation
%
%-Boltzmann distribution + trace of the observable

\newpage
% +++++++++++++++++++++++++
% =========================================================================
\bibliographystyle{alpha}
\bibliography{master_report}

% =========================================================================

\end{document}
