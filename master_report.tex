%    Template for seminar reports
% Seminar Current Topics in Computer Vision and Machine Learning
% Summer Semester 2015
% Computer Vision Group, Visual Computing Institute, RWTH Aachen

\documentclass[twoside,a4paper,article]{combine}
%\documentclass[11pt,twoside,a4paper]{article}

% =========================================================================
\usepackage[latin1]{inputenc}
\usepackage{a4}
\usepackage{fancyhdr}   
%\usepackage{german}    % Uncomment this iff you're writing the report in German
\usepackage{makeidx}
\usepackage{color}
\usepackage{t1enc}		% 	german letters in the "\hyphenation" - command
\usepackage{latexsym}	% math symbols
\usepackage{amssymb}    % AMS symbol fonts for LaTeX.
\usepackage{amsmath} 
\usepackage{physics}
\usepackage{graphicx}
\usepackage{pslatex}
\usepackage{ifthen}
\usepackage{tcolorbox}

\usepackage[T1]{fontenc}
\usepackage{pslatex}

\usepackage{psfrag}
\usepackage{subfigure}
\usepackage{url}

% =========================================================================

\setlength{\oddsidemargin}{3.6pt}
\setlength{\evensidemargin}{22.6pt}
\setlength{\textwidth}{426.8pt}
\setlength{\textheight}{654.4pt}
\setlength{\headsep}{18pt}
\setlength{\headheight}{15pt}
\setlength{\topmargin}{-41.7pt}
\setlength{\topskip}{10pt}
\setlength{\footskip}{42pt}

\setlength{\parindent}{0pt}

% =========================================================================

\graphicspath{
	{pictures/}
}

%%%
% We want also subsubsections to be enumerated
%%%
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\makeglossary
%\makeindex

% =========================================================================
\begin{document}

\include{titlepage}

\begin{abstract}
% +++++++++++++++++++++++++
% Insert your Abstract here
% +++++++++++++++++++++++++
\end{abstract}

\tableofcontents
\newpage
% =========================================================================


\section{Introduction}
%\cite{Ladd2010} General Possible Quantum Computer. 
%
%\cite{Boixo2016} The supremacy.
%
%\cite{Ronnow2014} Detect quantum speed up\\

Adiabatic quantum annealing can be considered as a special kind of quantum computation achieved by adiabatic evolution and quantum annealer is a device that can perform adiabatic quantum annealing. The idea of quantum computation was brought up in early 80s \cite{Feynman1982}. It indicated that a simulation of quantum phenomena is not always available on an classical computer, because the amount of resource required grows exponentially along with the size of the physical system. Instead, a quantum phenomena should be simulate by a computer making use of quantum property. The amount of resource required by the simulation is only proportional to the size of the physical system. \\

After this idea, an interesting topic for scientists is that how computation can benefit from storing, transferring and processing information with quantum properties. Quantum computation is expected to do more than just simulation of a quantum phenomena. Some quantum algorithm has been presented and shown to have a significantly speed-up compared to classical ones. The following are some example. Simon's algorithm can reduce the runtime for the task from an exponential time on a classical computer to a polynomial time on a quantum computer. \cite{Simon1994}. Furthermore, this also serve as the stepping-stone to Shor's Algorithm. Shor's Algorithm for factoring is one of the most well-know quantum algorithm. The time it takes to factor an n-digit number grows as a polynomial in n on a quantum computer. The time for the same task grows exponentially with n on a classical computer \cite{Shor1995}. The toughness of factoring problem is the basis of many cryptography techniques and information is usually encrypted and protected by a large semi prime number. Classical cryptography seems to break down and  quantum cryptography is under research as a response to this. The last one is Grover's search algorithm. To search an item in an unstructured list of size N costs a classical computer running time in an order of N. Grover's algorithm can solve the same task in the order of $\sqrt{N}$ \cite{Grover1996}. Not like Shor's algorithm, which has an exponential speed-up, Grover's algorithm has only a quadratic improvement. It is still a important algorithm because of its board applications, such as speedup the time required to solve NP-complete problems \cite{Cerf2000} \cite{Bennett1997}. \\

All algorithm mentioned above are expected to run on an universal quantum computer, which is usually referred to a machine based on quantum gate modal. However, building an quantum gate computer may be a challenging task. The main difficulty comes from the close box requirement \cite{Ladd2010}. The quantum system of a quantum computer should be isolated from its environment while being controlled. The quantum system which computation based on is so fragile that even a little amount of noise can cause harm to the system, known as decoherence. Furthermore, due to the environmental noise, the entropy of the system will keep increasing along with the time. Therefore, there must exist a way to coll the quantum system and maintain its quantum state. How to measure results is another a important issue, because the computational result makes sense to us only when we are able to measure it. Also, the possibility to scale up the system is crucial. A general quantum computing can be realised only when the above requirement are all fulfilled. Gate based computer is still struggling. Another approach to quantum computation is adiabatic quantum computer, which has more robustness than a gate based computer !!cite decoherence in AQC!!. There are debates on whether adiabatic quantum computer can be consider as a general quantum computer. It has shown that the model of adiabatic computation differs from the standard model of quantum computation within a polynomial time \cite{Aharonov2004}\cite{Farhi2000}. Some criticized that this only hold under ideal situation without noise. There are effective model for building a fault-tolerant gate based computer. By contrast, whether fault-tolerant is possible on quantum annealing computer remains unknown !!cite Classical signature of quantum annealing!!
\\

D-Wave System Inc. is a company which focus on building quantum annealing computer and the target problem of it is optimisation problem. Among optimisation problems, many can be cast into a problem of finding the minimum point of the cost function, which is equivalent to the ground state of a system of interacting spins. Finding such state remains still a computational hard problem. It is believed that if one can encode the cost function into the hardware, quantum annealing process can effectively achieve the goal\cite{Johnson2011}. During a quantum annealing process, the system start from a ground state and evolve in time with a problem Hamiltonian. In the end of evolution, the system are expected to be in the ground stated of the problem Hamiltonian, which is the solution to the optimisation problem. This is the reason why quantum annealing computer is sometimes considered specialized only in optimisation instead of universal quantum computer. D-wave System was founded  in 1999 and claimed to release the first commercial system in 2010 with 128 qubits. The latest system is D-Wave 2000Q with 2048 qubits. By emphasising on optimisation, quantum annealing computer has potential in the fields like machine learning, pattern recognition and computer vision. 
\\

Quantum annealer is built based on spin 1/2 system, which is Ising-model. The qubit in this system has two state, which is perfect for solving the 2-SAT problems. Start from an initial Hamiltonian, we slowly turn off this initial Hamiltonian and turn on the problem Hamiltonian. If the system is at its ground state and the whole process is slow enough, the system will also end at ground state with the problem Hamiltonian and we get th e solution of the 2-SAT problem. According to the adiabatic theorem this should work in a pure system, however, in the real case with environment. The system will be coupled with a heat bath. This heat bath may damage the coherent of the system evolution. \\ 

The outline of the report is as follows. In the section 2, the general idea of quantum annealer and the theory behind it will be introduced. Section 3 present the definition and the characteristic of optimisation problem. Section 4 provide the algorithms used for the simulation of a spin-system. In Section 5 and section 6, the simulation results of a ideal system and a system with Temperature effect will be discuss respectively. Section 7 is about some applications on machine learning and computer vision based on quantum annealing.
\\

% In section 3, I will introduce the optimization porblem, which believed to be a potential application of the quantum annealer. In section 4, I first simulate the quantum annealer without the heat bath. In section 5, I then simulate the quantum annealing process with hear bath. In the last section we try to discuss the possibility of using quantum annealing for machine learning.  \\

\newpage

\section{( Theory of ) Quantum Annealer}

%!A quantum annealer is a specialised machine that solves optimisation problems by evolving from a know initial configuration to the ground state of a Hamiltonian encoding a given problem.
%
%
%\cite{Das2008} 
%\cite{Matsuda2009}
%\cite{Santoro2006}
%\cite{Denchev2015} what is the computational value of finite range tunnelling




\subsection{Quantum Annealing and Adiabatic Quantum Computation}
%\cite{Boixo2014} A compare of quantum annealing and simulated annealing \\
%\cite{QABoixo2016} Multi qubit tunneling \\
%\cite{Jin2013} quantum decoherence \\
%\cite{Smolin2013} Classical signature of quantum annealing p2 \\

Before going into quantum annealing, it's worth mentioning first the reason using annealing technique in general and the difference between simulated annealing and quantum annealing. First, the reason using annealing technique is because of the properties of the combinatorial optimisation problem. The goal of this kind of problem is to find an optimal solution among a finite set of possible configuration. The set of solution is discrete. A brute-force search for the optimal one is not practical on a classical computer when the problem size is large. To reduce runtime, one approximate method is annealing. If one can encode the problem into a cost function and correspond the minimum value of the cost function to the solution of the optimisation problem, annealing algorithm can help find the lowest point in the landscape constructed by the cost function.  \\

Annealing algorithm is inspired by the annealing in metallurgy and material science. It is a heat treatment that heating a material up to a temperature that recrystallisation can occurs, and cooling it down. Recrystalisation process lower the free energy of the crystals \cite{Schader2012}. It's an ancient technique to improves the properties of material and makes it more workable. \\

Following this process, simulated annealing is a optimisation method for approximating global minimum of a given function. In simulated annealing, the system starts from a high energy state and by lowering the temperature very slowly, the system is expected to end up in the lowest energy state or at least a close approximation. However, there are two situation which may turn simulated annealing into a inefficient algorithm. First, if the landscape is too rugged and the energy barrier around local minima is too high, the system may get stuck in one of the local minima which is apparently not the optimal solution. These deep barrier may trap the system for a very long time during the evolving. The second problem is the complexity. The number of possible solutions grows exponentially with the problem size, because $n$ Ising spins have $2^n$ configurations. Simulated annealing can only go through one configuration at a time. !Unless there is a gradient can guide the system toward the global minimum from any point of the configuration space, simulated algorithm can do no better than a random search one. \\



On the other hand, quantum annealing may become more efficient and perform better than simulated annealing when facing these two situations, because the phenomenon of quantum tunnelling help explore more search space. More precisely, quantum mechanics allows the system to tunnel through very high barriers in a classically inaccessible path once barriers are narrow enough. !Furthermore, if the Hamiltonian of a given problem is applied properly to the system, the quantum mechanical wave function can delocalise over the whole search space, that is, it has the ability to see the entire landscape during annealing. In contrast, simulated annealing can only search the configuration space randomly and escape from local minimum by the aid of thermal fluctuations. Therefore, quantum annealing is superior to simulated annealing algorithm in these two aspect. However, empirically quantum annealing does not always improve the search process and it has its own limitation. Some counter cases has been observed for k-SAT problems \cite{Battaglia2005}. In brief, although for both algorithms the system may stop evolving at a local minimum state instead of the global minimum, if the barrier constraining the local minimum is narrow enough, the quantum tunnelling can give the system have a chance to tunnel through the barrier and keep evolving toward the true minimum. An illustration is shown in Fig.\ref{diff_qa_ca}. \\ 

\begin{figure}[h]
	\centering
	\includegraphics[height=150pt]{qaca.png}
	\caption{The difference between QA and CA. Image from \cite{Das2008}. }
	\label{diff_qa_ca}
\end{figure}



 

%The idea of annealing in general is to find the lowest-energy state, which sometime cast into a problem that try to find the lowest value of a cost function. When performing classic annealing, the system start from a random state in a high temperature. Then the temperature decrease, simulated algorithm choose the new state by some criteria. The system evolve by reducing the energy, and the result is usually end up in a local minima instead of global minima. !!The choose criteria\\

The main dynamics in quantum annealing process is quantum fluctuation. To achieve this, the system is first prepared in the ground state of the initial Hamiltonian, which is a product states of all spin states. Next, to bring in quantum fluctuation, a strong transverse field is applied to they system. During the annealing, the transverse field is slowly turned off while the problem Hamiltonian is slowly turned on. If this procedure progress slowly enough, the system will stay still in the ground state. At the end of the annealing process, the transverse field is complete off and the system should have evolved to the ground state of the problem Hamiltonian that encoded the given optimisation problem. The Hamiltonian used here is from classical Ising model and will be introduced in the next paragraph. How to encode an optimisation problem into a Ising model Hamiltonian will be presented in detail in Section \ref{The Mapping of Hamiltonian}. \\

%\begin{equation}
%H_{problem}=\sum_{i,j}^N \{J^x \sigma^x_i \sigma^x_j+J^y \sigma^y_i \sigma^y_j+J^z \sigma^z_i \sigma^z_j \}+ \sum_{i}^N \{h^x \sigma^x_i+h^y \sigma^y_i+h^z \sigma^z_i\}
%\end{equation}

%\begin{equation}
%H_{Ising}=\sum_{i,j}^N \{J^z \sigma^z_i \sigma^z_j \}+ \sum_{i}^N \{h^z \sigma^z_i\}
%\end{equation}

\begin{equation}
H_{Ising}=\sum_{i,j}^N \{J_{ij} \sigma_i \sigma_j \}+ \sum_{i}^N \{h_i \sigma_i\},
\end{equation}

where $\sigma_i$ stands for Pauli matrices 

\begin{equation}
\label{pauli_matrix}
\begin{split}
&\sigma^x= \begin{pmatrix}
			0&1\\1&0
		  \end{pmatrix}\\
&\sigma^y=\begin{pmatrix}
			0&-i\\i&0
		  \end{pmatrix}\\
&\sigma^z=\begin{pmatrix}
			1&0\\0&-1
		 \end{pmatrix}.
\end{split}
\end{equation}

!The Hamiltonian of classical Ising model with N spins can be written as above. This model is broadly used in statistical physics to show a phase transition. The formula describe a set of lattice sites with discrete variables that represent the atomic spins with either up or down state. The model allows interaction between neighbours according to factor $J_{ij}$. If $J_{ij}<0$, the interaction is called ferromagnetic and the neighbour spins have higher probability to align parallel. In contrast, if $J_{ij}>0$, it is antiferromagnetic and the neighbour spins favour opposite states. Similarly, An external field $h_i$ interacts with each spin. The spin tends to have negative direction when $h_i>0$, and positive direction when $h_i<0$. Obviously, finding the configuration of the ground state under a given parameter set is the main and difficult task after one maps a optimisation problem into the Hamiltonian. \\

Based on classical Ising model, the Hamiltonian used by quantum annealing process can be written as follow

\begin{equation}
\label{Hamiltonian_set}
\begin{split}
&H(t)=(1-\frac{t}{T} )H_{init}+(\frac{t}{T})H_{problem} \\
&H_{init}= \sum_{i=1}^{N}h_i^x \sigma_i^x\\
&H_{problem}= \sum_{i,j}^N J_{ij}^z \sigma^z_i \sigma^z_j + \sum_{i}^N h_i^z \sigma^z_i,\\
\end{split}
\end{equation} 

where t is the current timestep and T is toatal annealing time.\\

The key feature of $H_{init}$ is that it should be straightforward to construct and its ground state is easy to find. A transverse field in x-direction is the choice here. The ground of $H_{init}$ is $\ket{x_1=0}\ket{x_2=0}\ket{x_3=0}\dots\ket{x_n=0}$. This state can be written in our computational basis, z-basis, in a superposition of all $2^n$ basis vectors and !the states $\ket{z_i}$ are eigenstates of the i-th spin in z-direction,

\begin{equation}
\ket{x_1=0}\ket{x_2=0}\ket{x_3=0}\dots\ket{x_n=0}=\frac{1}{2^{n/2}}\ket{z_1}\ket{z_2}\dots\ket{z_n}.
\end{equation}

!$H_{problem}$ is spanned by the computational basis, z-basis. The ground state of $H_{problem}$ can be found only when $z_1, z_2 \dots z_n$ in the superposition of the states $\ket{z_1}\ket{z_2}\dots\ket{z_n}$ can satisfy all clauses of the given SAT problem. \\


%Quantum annealing does not evolve by reducing temperature. The system is affect by two Hamiltonian, one is $H_i$, which is our starting Hamiltonian, and the requirement of this Hamiltonian is that the ground state of this is easy to prepared. On the other hand, $H_p$ is the problem Hamiltonian. We map the problem in to $H_p$. The idea is that, if we start from the ground state of $H_i$, and then the $H_i$ is turned off and the $H_p$ is turned on sufficiently slow. The system will end up in the ground state of $H_p$, which is the solution we want to find.\\

At $t=0$ the annealing process starts and the quantum fluctuation dominate the $H(t)$. The system is easy to initialise because of the feature of $H_{init}$. According to adiabatic theorem, which will be introduced in section \ref{adiabatic theorem}, if the total annealing time is long enough, the system can evolve adiabatically and remain in the ground state. As a result, when $t=T$ and $H(t)$ shifts from $H_{init}$ to $H_{problem}$ completely, the final state is in the ground state of $H_{problem}$. It should be noted that by using $(1-\frac{t}{T})$ and $\frac{t}{T}$ as the relation of $H_{init}$ and $H_{problem}$ in Equation \ref{Hamiltonian_set}, it implies the annealing proceeds with a linear time scheme. Indeed, there are more general time schemes can be implemented for the annealing process rather than a linear one. For simplicity's sake, the linear time scheme is applied to the annealing process in this report. A illustration of this shown in Figure \ref{linear_timescheme}. 

\begin{figure}[h]
	\centering
	\includegraphics[height=150pt]{doge.jpeg}
	\caption{Linear time scheme. (It should be drawn here later on.) }
	\label{linear_timescheme}
\end{figure}


the time scheme can be used is not \\

The adiabatic quantum annealing algorithm is presented as follow \cite{Farhi2000}:\\

\begin{tcolorbox}[title=Quantum Annealing Algorithm]
1. Initialise the system to the ground state of $H_{init}$. The ground state of $H_{iniy}$ is designed in a way that it is simple to construct.\\
%\begin{equation*}
%	H_i=\sum_{i=1}^{N}h_i^x \sigma_i^x
%\end{equation*}   

2. Set up $H_{problem}$ according to a given problem. Combine $H_{init}$ and $H_{problem}$ to Build $H(t)$. \\
%\begin{equation*}
%	H_{problem}=\sum_{i,j} \{J^x \sigma^x_i \sigma^x_j+J^y \sigma^y_i \sigma^y_j+J^z \sigma^z_i \sigma^z_j \}+ \sum_{i} \{h^x \sigma^x_i+h^y \sigma^y_i+h^z \sigma^z_i\}
%\end{equation*}   

3. Evolve the system by computing time-dependent Schrödinger equation for time $t$ according to a given time scheme. \\
%\begin{equation*}
%	T:linear 
%\end{equation*}

5. The system will end up in the ground state of the $H_p$, if the total annealing T is long enough.\\

6. Measure the coefficient of $z_1,z_2 \dots z_n$. The result of the measurement will be the solution to the given problem, if it is satisfiable. If the given problem is not satisfiable, the system will end up in a configuration that minimise the number of violated clause.\\

\end{tcolorbox} 

It is worthwhile to mention the inherent difference between simulated annealing and quantum annealing. In the ideal case, the simulated annealing has some random factor when choosing the beginning point and also during the process, but quantum annealing does not have this randomness. Therefore, in principal, a idea quantum annealing will either always find the solution, or never, which leads to a bimodal distribution. Classical annealing starts from a random state, on the other hand, may have a uni-modal result. This is considered as a main criterion to distinguish between simulated annealing and quantum annealing\cite{Boixo2013}. Based on this, D-wave System Inc. declared that there quantum annealer have this quantum characteristic, but the some opposite claimed that by picking same random state, the simulated annealing could also show a bimodal result \cite{Smolin2013}. \\

There two crucial problems for quantum annealing. First, the length of the total annealing time T. How long should it be to solve an interesting problem? Second, when the given problem is complicated, the energy gap between each eigenstate may be very close. How would the gap between the ground state and first excited state effect the annealing process? This are two aspect that will be investigated in Section \ref{result_ideal} and Section \ref{result_temp}.
% Quantum annealing is a technique to find the ground state of a optimisation problem. different from the thermal annealing is that can tunnel through the barrier. 



\subsection{Adiabatic Theorem}
%\label{adiabatic theorem} \cite{Sarandy2004}

The adiabatic theorem mainly serves as the basis of quantum annealing. The concept is that if the Hamiltonian of an eigenstate alters gradually, then the state will remain an eigenstate at later times while the !eigenenergy evolves continuously. The relation between gradually varying Hamiltonian and adiabatic behaviour can be demonstrated as follow. Equation \ref{tdse} states a quantum system which evolves unitarily.

\begin{equation}
\label{tdse}
-t\frac{\partial}{\partial t}\ket{\Psi (t)} = H(t)\ket{\Psi(t)},
\end{equation}

where $H(t)$ is the time-dependent Hamiltonian and $\ket{\Psi(t)}$ is a quantum state. Assume the spectrum of $H(t)$ is nondegenerate. As a result, an basis can be defined as 

\begin{equation}
	H(t)\ket{n(t)}=E_n(t)\ket{n(t)},
\end{equation}

where $\ket{n(t)}$ is the set of eigenvector !chosen to be orthonormal. Suppose the time-dependent Hamiltonian can be diagonalised by unitary transformation. 

\begin{equation}
\label{unitary}
H_D(t)=U^{-1}(t)H(t)U^{1}(t),
\end{equation} 

where $H_D{t}$ stands for the diagonalised Hamiltonian and $U(t)$ is its unitary transformation. By multiplying $U^{-1}$ to Equation \ref{tdse}, it can be rewritten as 
 
\begin{equation}
H_D\ket{\Psi}_D = i \frac{\partial}{\partial t}\ket{\Psi}_D - i \frac{\partial}{\partial t} U^{-1} \ket{\Psi},
\end{equation}

where $\ket{\Psi}_D \equiv U^{-1}\ket{\Psi}$ !is the state of the system in the basis of eigenvector of $H(t)$. If $H(t)$ changes slowly enough in time, $\frac{\partial H(t)}{\partial t}$ approximates zero. It can then be assumed that the unitary transformation, $U(t)$, and its inverse, $U^{-1}(t)$ are !slowly varying operator, which leads to 

\begin{equation}
H_D(t)\ket{\Psi(t)}_D = i \frac{\partial}{\partial t} \ket{\Psi(t)}_D.
\end{equation}

Consequently, the system can evolve separetely in each energy eigenstate, because $H_d(t)$ is diagonal. In other words, this assures that if the quantum system is in an instantaneous eigenstate of $H(t)$ at a certain point of time, it will remain in this eigenstate at all time. \\ 

%Once the annealing time is long enough. We should be able to find the ground state.

\subsection{Landau-Zener Transition} %theorem

According to Landau-Zener formula, another constraint on quantum annealing process is the distance between the ground state and the first excited state. Landau-Zener formula gives out the probability of a diabatic transition from a lower energy eigenstate to a higher energy eigenstate. Although adiabatic theorem implies that no diabatic transition will occur if $H(t)$ in Equation \ref{Hamiltonian_set} could vary infinitely slow, this is not the case for real system with a finite annealing time. Once the system evolve in a finite annealing time from a lower energy eigenstate, the probability of finding the system in a higher energy state in the future can be determined by Landau-Zener formula.\\

$P_{adiabatic}$ is the probability of the system remaining ground state and evolve adiabatically. $P_{diabatic}$ is the probability of diabatic transition. The relation between these two is $P_{adiabatic} = 1 - P_{diabatic}$. By Landau-Zener formula, $P_{diabatic}$ is 

\begin{equation}
\begin{split}
P_{diabatic}(T) = \exp({\frac{-2\pi\Delta_{min}^2 T}{\alpha\Gamma_0}}),
\end{split}
\end{equation}
where $T$ is the total annealing time in Equation \ref{Hamiltonian_set}, $\Delta$ is the minimum gap between two energy eigenstates which are always referred to the gap between the ground state and the first excited state in this report, $\alpha$ is the !relative slope of the two states, and $\Gamma_0$ is the initial strength of the transverse field which is set in $1$ in this report. An illustration of Landau formula can be found in Figure. \ref{landau}. \\

\begin{figure}[h]
	\centering
	\includegraphics[height=150pt]{doge.jpeg}
	\caption{a graph show the landau formula parameter. (it should drawn here later on.) Image from\cite{Santoro2002}. }
	\label{landau}
\end{figure}

The probability of a successful quantum annealing is thus confined by the minimum gap, $\Delta_{min}$ and the annealing time $T$ correspondingly. The relation between the successful probability and $\Delta_{min}$ can indeed be investigated with a fixed annealing time $T$. Further detail will be presented in Section \ref{result_ideal} and Section \ref{result_temp}. 




%The probability of finding the ground state is depend on the temperature and the gap.

%\subsection{Annealing Time scheme}
%From the previous sector, we can noticed that the time scheme for $\lambda$ should be chosen and set. Here we use linear scheme for $\lambda$.

\newpage

\section{Optimisation Problem}
\subsection{Combinatorial Optimisation}

Combinatorial optimisation problem is one of the most important task people want to solve. If one need to find the best choice among all available options consist of many independent factors, it can be considered as a combinatorial optimisation problem. Some practical examples are like travelling salesman problem, closure problem and assignment problem. Take travelling salesman problem for instance, there are N cities located randomly in an area and the goal is to plan an trip that visit every city once in the shortest travel distance. A straightforward approach is to try out all permutations and find the shortest path. The running time for this brute-force search is $\mathcal{O}(n!)$ , which means it grows as the factorial of the number of cities and the search space easily becomes too large for this approach to be feasible even for a relatively small N ($N \sim 30$) \cite{Santoro2006}. \\

Many combinatorial optimisation problems can be cast into a problem of minimising a given cost function $H(S_1,S_2,S_3,...S_N)$ with respect to N variables $S_1$, $S_2$ to $S_N$ \cite{Das2008}. The aim is to determine a set of values for the variables that yield the minimum value of the cost function. Combinatorial optimisation problem often has the following properties. The optimal solution is searched from a finite set of objects. The set of objects is discrete. Brute-force search is usually not feasible. Besides, even some problems with continuous variables can be reduced to combinatorial problems \cite{Papadimitriou1984}. \\

In the field of computer science, the complexity of a computational task are classified by the running time needed by the fastest algorithm relative to the size of the task. If a given task can be solved in polynomial time by using polynomial bound resource on classical computer, it is considered to be in the class P. Although a P problem does not necessary means an easy problem, a polynomial bond on the evaluation time implies its difficulty won't grow exponentially along with the problem size. Unfortunately, not all cases fall in this class, and some of the important ones are fall outside, like the travelling salesman problem mentioned above.\\

A given task is said to be in the class NP, if the solution can be found in polynomial time by non-deterministic Turing machine. A deterministic Turing machine can only perform one action for a given situation according to its set of rules, while a non-deterministic Turing machine can perform more than one action for a given situation and if any of them find the solution, it achieves its goal. In other word, a non-deterministic Turing machine has the potential to explore exponentially many paths in parallel with time and check if any on of them can solve the task. It's clearly that deterministic Turing machine is a weak version of non-deterministic one. Therefore class P is also included inside the class NP as a special case. Not like a problem in class P, the polynomial-time algorithm know to be exist, problems in class NP are believed to require super-polynomial time. \\

There is a subset inside class NP called NP-complete, which is considered to be the hardest set among class NP and any NP problem can be reduced into class NP-complete with a polynomial overhead. It is general believed that if one can find an polynomial algorithm to solve an NP-complete problem, all problems in class NP can also be solved by this polynomial algorithm.  The task fall inside either class NP or class NP-complete are considered to be hard, because non-deterministic Turing machine is not yet possible to be simulated by a deterministic Turing machine without an exponential growth of execution time. \\ 

Indeed there are algorithms can solve some easy optimisation problems in polynomial time. For harder cases like class NP-complete, although one cannot find the exact solution in a polynomial time, there are still some specialised algorithm that can approximate the solution in an polynomial time. The downside is that these algorithm are very problem specific, which means a success in one NP-complete problem does not ensure the success when attacking other NP problems by using the same algorithm. \\

\subsection{Boolean Satisfiability Problem}

The boolean satisfiability problem, abbreviated as the SAT problem, is a task of checking whether a given set of boolean formula can be satisfied. To be more specific, the goal here is to find a configuration of true or false for all variables in a given Boolean formula and it leads the final evaluation of the formula to true. The formula is called satisfiable when this is the case. On the other hand, the formula is not satisfiable, if the evaluation is false for all possible configuration.  \\

The k-SAT problem with k>2 has been proven to be in class NP-complete, which implies that no algorithm can efficiently solve SAT problems in polynomial time. The parameter k defines the upper limit of the number of variables in one clause. An example is 

\begin{equation*}
 (x_1\lor x_2\lor x_3)\land (x_4\lor x_5\lor \neg x_6),
\end{equation*}

where $\land$,$\lor$, and $\neg$ state for logical and, logical or, and logical not respectively. This is a Boolean formula in 3-SAT form, in which has two clauses and each clause consist of at most 3 variables. This formula is satisfiable because the final evaluation is true while one of the variable is true among $x_1, x_2, x_3$ in the first clause ,and one of the variable is true among $x_4, x_5$ or $x_6$ is false in the second clause. \\

In this work, 2-SAT problem is the target task to understand. Similarly, the definition of a 2-SAT problem is that one clause can contain at most two variables. In contrast to those more general ones which are known to be NP-complete, 2-SAT problem can be solved in polynomial time. A detail should be mentioned is that the 2-SAT problems studied in this work all has only one unique configuration which leads the evaluation to be true. In other words, there is no degenerate ground states for the problem. \\
 
\subsection{The Mapping of Hamiltonian}
\label{The Mapping of Hamiltonian}

A 2-SAT problem can only be solved when its Boolean formula is mapped in to an Ising Hamiltonian. In Ising model, an spin can take either 1 or -1 as its value, so it is straightforward to correspond spin value 1 for logical true and spin value 0 for logical false. \\

The following simple example can help demonstrate the mapping of Hamiltonian. 

\begin{equation*}
(x_1\lor x_2)\land (x_3\lor \neg x_4)
\end{equation*}

The whole statement is true if in the first clause, either $x_1$ or $x_2$ is true and in the second clause, $x_3$ is true or $x_4$ is false. The mapping is as follow:

\begin{table}[h!]



\begin{center}
	\begin{tabular}{|c|c|c|c|c|}
		
		\multicolumn{5}{c}{2-SAT Variables}\\
		\hline
		  & T & T & T & F \\
		\hline
		\hline
		$x_1$ & 1 & 1 & 0 & 0 \\
		\hline
		$x_2$ & 1 & 0 & 1 & 0 \\
		\hline
	\end{tabular}
	\quad
	$\Rightarrow$
	\quad
	\begin{tabular}{|c|c|c|c|c|}
		\multicolumn{5}{c}{Ising variables}\\
		\hline
		 & T & T & T & F \\
		\hline
		\hline
		$\sigma_1$ & 1 & 1 & -1 & -1 \\
		\hline
		$\sigma_2$ & 1 & -1 & 1 & -1 \\
		\hline
		$m=\sigma_1+\sigma_2$ & 2 & 0 & 0 & -2 \\
		\hline
	\end{tabular}
\end{center}

\begin{center}
	\begin{tabular}{|c|c|c|c|c|}

		\hline
		& T & T & T & F \\
		\hline
		\hline
		$x_3$ & 1 & 1 & 0 & 0 \\
		\hline
		$x_4$ & 0 & 1 & 0 & 1 \\
		\hline
		
	\end{tabular}
		\quad
		$\Rightarrow$
		\quad
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			& T & T & T & F \\
			\hline
			\hline
			$\sigma_3$ & 1 & 1 & -1 & -1 \\
			\hline
			$\sigma_4$ & -1 & 1 & -1 & 1 \\
			\hline
			$m=\sigma_3-\sigma_2$ & 2 & 0 & 0 & -2 \\
			\hline
		\end{tabular} 

\end{center}
\caption{A way to map the 2-SAT clauses to the Ising variables.}
\end{table} 

In order to encode the solution of the 2-SAT problem into the ground state of the Ising Hamiltonian, an Ising Hamiltonian can be built up with a magnetisation $m$. The Hamiltonian with magnetisation $m$ is given by 

\begin{equation}
\begin{split}
H & = m \cdot (m-2)\\
  & = \sigma_i^2 + \sigma_j^2 + 2\sigma_i \sigma_j -2\sigma_i -2\sigma_j\\
  & = 2\sigma_i \sigma_j -2\sigma_i -2\sigma_j + const.\\
\end{split}
\end{equation}

After comparing with the Hamiltonian mentioned in previous section, one can correspond $h_i^z$, $h_j^z$, and $J_{ij}$ to 2, 2, and -2 respectively. Then one just repeats this procedure for all clauses. After this mapping is complete, the original 2-SAT problem can be solved with quantum annealing technique, because the ground state of the Ising Hamiltonian can be transformed back to the solution of the 2-SAT problem. \\  


\newpage



\section {Simulation Algorithm}

\subsubsection{Time-Dependent Schrodinger Equation and Quantum Spin System}

The whole quantum annealing process can be described by time-dependent Schrödinger equation ( Equation \ref{tdse}), that is by solving time-dependent Schrödinger equation one can determine the final state of the system. The solution of this is 
\begin{equation}
\begin{split}
\Psi(t+\tau)&=U(t+\tau,t)\Psi(t)\\
&=\exp(i\int_{t}^{t+\tau}H(\tau)d\tau)\Psi(t),
\end{split}
\end{equation}
!where $\tau$ is the time step, and $U(t+\tau)$ is a unitary matrix that transform system from $t$ to $t+\tau$. In order to conduct numerical computation and ensure the unitary of the evolution operator, time is discretised to small time step $\tau$ and $H(\tau)$ is assumed to be piecewise constant within these time steps. Therefore, the size of the time step should be small enough to keep $H(\tau)$ piecewise. On the other hand, if the size is too small, the computational time will be too long. A proper length should be tested out before going into further simulation. \\

The $\Psi$ is defined by the direct product state of the N single spin states in z-basis, which is the computational basis used in this report. Each single spin has two possible states, $\ket{0}$ or $\ket{1}$, which correspond to, $\ket{\uparrow}$ or $\ket{\downarrow}$ . The relation between them is defined as 

\begin{equation}
\ket{0}=\ket{\uparrow}=\left(\begin{array}{c} 1\\0 \end{array}\right), \ket{1}=\ket{\downarrow}=\left(\begin{array}{c} 0\\1 \end{array}\right)
\end{equation}


Thus, the single spin state can be written as a linear superposition of theses two basis state, 

\begin{equation}
\ket{\psi} = a(0)\ket{0} + a(1)\ket{1},
\end{equation}

where $a(0) and a(1)$ are the coefficient of the amplitude of each state. Furthermore, a N-spin system is made up of $2^N$ state vectors and it can be written as 

\begin{equation}
\label{psi_in_compuationalbasis}
\Psi = a(00\dots0)\ket{00\dots0}+a(00\dots1)\ket{00\dots1}+\dots+a(11\dots0)\ket{11\dots0}+a(11\dots1)\ket{11\dots1},
\end{equation}

where $a(00\dots0), a(00\dots1)\dots, a(11\dots1)$ are the coefficients of each components. It should fulfil the normalisation requirement, $\braket{\Psi}{\Psi} = 1$. That is, the following equation should hold. 

\begin{equation}
\sum_{\sigma_1,\sigma_2, \dots, \sigma_N=\ket{0},\ket{1}} |a(\sigma_1,\sigma_2, \dots, \sigma_N)|^2 = 1. 
\end{equation}

One may noticed that the format of basis vectors in Equation \ref{psi_in_compuationalbasis} is similar to the format of binary number notation. !If the first is considered to be the least significat bit of an integer, the basis vector can be denoted as a integer ranging from 0 to $2^N-1$. \\
%According to Equation \ref{psi_in_compuationalbasis}, it is 

It is clear that the memory request for the computation will increase exponentially with the size of the quantum system, which may soon make the computation not practical on a classical computer. However, it turns out there is a simple way to calculate the result without accessing to the entire matrix every time. Namely, during applying $\sigma^x_i, \sigma^z_i, and \sigma^z_i\sigma^z_j$ to $\ket{\Psi}$ the computation can be reduced to series of $2x2$ and $4x4$ matrix calculation instead of a calculation of entire $2^N x 2^N$ matrix. After applying Hamiltonian operator to Equation \ref{psi_in_compuationalbasis}, the state vectors become 

\begin{equation}
\begin{split}
\ket{\Psi'} &= \sigma^\alpha_i \ket{\Psi} \\
			&= a'(00\dots0)\ket{00\dots0}+a'(00\dots1)\ket{00\dots1}+\dots+a'(11\dots0)\ket{11\dots0}+a'(11\dots1)\ket{11\dots1},
\end{split}
\end{equation}

If the operator is $\sigma^x_i$, it interchanges state up and state down, which corresponds to a swap of a pair component of $\Psi$. 

\begin{equation}
\begin{split}
a'(\bullet \cdots \bullet 0 \bullet \cdots \bullet) = +a(\bullet \cdots \bullet 1 \bullet \cdots \bullet)\\
a'(\bullet \cdots \bullet 1 \bullet \cdots \bullet) = +a(\bullet \cdots \bullet 0 \bullet \cdots \bullet)
\end{split}
\end{equation}

If the operator is $\sigma^y_i$, according to Equation \ref{pauli_matrix}, the coefficient has the relation 

\begin{equation}
\begin{split}
a'(\bullet \cdots \bullet 0 \bullet \cdots \bullet) = -i\times a(\bullet \cdots \bullet 1 \bullet \cdots \bullet)\\
a'(\bullet \cdots \bullet 1 \bullet \cdots \bullet) = +i\times a(\bullet \cdots \bullet 0 \bullet \cdots \bullet)
\end{split}
\end{equation}

If the operator is $\sigma^z_i$, it reverses the sign of all coefficients of $\ket{\Psi}$ for which the $i-th$ bit of the vector has value 1, which yields

\begin{equation}
\begin{split}
a'(\bullet \cdots \bullet 0 \bullet \cdots \bullet) = +a(\bullet \cdots \bullet 1 \bullet \cdots \bullet)\\
a'(\bullet \cdots \bullet 1 \bullet \cdots \bullet) = -a(\bullet \cdots \bullet 0 \bullet \cdots \bullet)
\end{split}
\end{equation}

Similarly, applying two spin operator $\sigma^z_i\sigma^z_j$ leads to a sign interchange between coefficients of $\ket{\Psi}$ for which the $i-th$ bit and the $j-th$ bit of the vector has different valus. Hence it is 

\begin{equation}
\begin{split}
a'(\bullet \cdots \bullet 0 \bullet \cdots \bullet 0 \bullet \cdots \bullet) = +a(\bullet \cdots \bullet 0 \bullet \cdots \bullet 0 \bullet \cdots \bullet)\\
a'(\bullet \cdots \bullet 1 \bullet \cdots \bullet 0 \bullet \cdots \bullet) = -a(\bullet \cdots \bullet 1 \bullet \cdots \bullet 0 \bullet \cdots \bullet)\\
a'(\bullet \cdots \bullet 0 \bullet \cdots \bullet 1 \bullet \cdots \bullet) = -a(\bullet \cdots \bullet 0 \bullet \cdots \bullet 1 \bullet \cdots \bullet)\\
a'(\bullet \cdots \bullet 1 \bullet \cdots \bullet 1 \bullet \cdots \bullet) = -a(\bullet \cdots \bullet 1 \bullet \cdots \bullet 1 \bullet \cdots \bullet)\\
\end{split}
\end{equation}

Please note that the operation mentioned here are done in place. Namely, instead of creating and using extra unnecessary vector, these operation manipulate the necessary component in the state vector $\ket{\Psi}$. Hence, $\ket{\Psi'}=H\ket{\Psi}$ can be calculated easily. This approach serves as the basis of the algorithm introduced in following subsections. \\
%\begin{equation}
%i\hbar\frac{\partial \psi}{\partial t}=H\psi
%\end{equation}
%\begin{equation*}
%H=(1-\lambda)H_{init}+\lambda H_{problem}
%\end{equation*}
%\begin{equation*}
%H_{init}=\sum_{i} h^x_{init} \sigma^x_i
%\end{equation*}

\subsection{Algorithm for Ideal Case}
So here i am going to try out two algorithm. First is full diagonalization, which i use Lapack to solve the eigensystem. Product formula break the evolution into small operation and make the computation of large system possible.

\subsubsection{Full Diagonalisation}

Base on the fact that Hamiltonian H is an Hermitian operator formed by a $2^N \times 2^N$ matrix, it has a complete set of eigenvectors and eigenvalues. This implies that the matrix H can be transformed into a diagonal matrix $\Lambda$ spanned by its eigenvalues and the unitary matrix $V$ of its eigenvectors. The transformation is given by $H = V\Lambda V^\dagger$. Thus the time evolution can be calculated as $U(\tau) = exp(-i\tau H) = V exp(-i\tau \Lambda) V^\dagger$. If $V$ and $\Lambda$ are obtained by diagonalising H, time evolution becomes a series of simple matrix multiplication. Since diagonalisation is a typical matrix problem, there are already standard software library can be made use of. \\

The element of H can be computed by repeatedly applying Hamiltonian H to basis vector $\ket{\Psi}$, that is $\ket{\Psi'} = H\ket{\Psi}$. ! First, $\ket{\Psi}$ is set to be $(1,0,\dots,0)^T$. $\ket{\Psi'} = H\ket{\Psi}$ will then result in the first column of the matrix H. Same calculation for  $(0,1,0,\dots,0)^T$ will lead to the second column of the matrix H and so forth. Consequently, the matrix H will be determined. \\

The main drawback of this approach is the limitation on scaling. !Whlie diagonalising the matrix H, the memory request and CPU time of the standard algorithm scale as $D^2$ and $D^3$ with $D=2^N$. Due to the exponential growth of the matrix size $D$, full diagonalisation approach fits only small quantum system. Therefore, this approach serves mostly as a tool to valid the correctness of other algorithm when solving with time-dependent Schrödinger equation\cite{DeRaedt2004}.
%Use Lapack to diagonalize the H for $\Psi_{t} = e^{iH\tau} \Psi_{t-\tau}$. 

\subsubsection{Suzuki-Trotter Product Formula}
Check Reference \cite{DeRaedt2004}  E. Suzuki-Trotter Product-Formula Algorithms

The Suzuki-Trotter product formula approach is the one that will suit for a larger quantum system. Suzuki-Trotter product formula approximate a unitary matrix exponentials by decomposing the matrix properly, that is 

\begin{equation}
\label{suzuki-trotter product formula}
\begin{split}
U(t) &= \exp(-i\tau H)\\
&=\exp(-t\tau(H_1+H_2+\dots+H_K))\\
&=\lim_{m \to \infty}(\prod_{k=1}^{K}\exp(-i\tau H_k/m))^m
\end{split}
\end{equation}

If the time step $\tau$ is small enough, a good approximation of $U(t)$ suggested by Equation \ref{suzuki-trotter product formula} is 

\begin{equation}
\label{suzuki-trotter product formula 1st}
\tilde{U}_1(\tau) = \exp(-i\tau H_1) \exp(-t\tau H_2) \dots \exp(-t\tau H_K).
\end{equation} 
 
 !The Taylor series of $U(t)$ and $\tilde{U}_1(t)$ are identical up to first order in $\tau$. Thus $\tilde{U}_1(t)$ is the first order approximation of $U(t)$. Furthermore, if all $H_i$ in Equation \ref{suzuki-trotter product formula 1st} are Hermitian, $\tilde{U}_1(t)$ is unitary by construction and hence the algorithm based on Equation \ref{suzuki-trotter product formula 1st} will be unconditionally stable. The accuracy of the algorithm can be increased by applying second order approximation and it is defined by 
 
\begin{equation}
\begin{split}
\tilde{U}_2(\tau) &= \tilde{U}_1(\frac{\tau}{2})^T\tilde{U}_1(\frac{\tau}{2}) \\
&=\exp(\frac{-t\tau H_K}{2})\exp(\frac{-t\tau H_{K-1}}{2})\dots\exp(\frac{-t\tau H_1}{2})\exp(\frac{-t\tau H_1}{2})\dots\exp(\frac{-t\tau H_K}{2})
\end{split}
\end{equation} 

Here $\tilde{U}_2(\tau)$ is again unitary by construction as $\tilde{U}_1(\tau)$. !For a fixed accuracy in this $\tilde{U}_2(\tau)$ approximation, memory required and CPU time scale as $\order{D}$ and $\order{\tau^{1+1/2}D}$ respectively. The crucial step of this algorithm is to choose $H_i$ properly in a way that it is easy enough to calculate there matrix exponential, $\exp(\frac{-t\tau H_1}{2})\dots\exp(\frac{-t\tau H_K}{2})$, efficiently.  \\

The Hamiltonian used in this report is listed in Equation \ref{Hamiltonian_set}. One efficient way to construct second order approximation is 

\begin{equation}
\label{tilde_U}
\begin{split}
\tilde{U}(\tau) = \exp(\frac{-i\tau H_{\sigma_\alpha}}{2})\exp(\frac{-i\tau H_{\sigma_x \sigma_x}}{2})\exp(\frac{-i\tau H_{\sigma_y \sigma_y}}{2})\exp(-i\tau H_{\sigma_z \sigma_z})\\\exp(\frac{-i\tau H_{\sigma_y \sigma_y}}{2})\exp(\frac{-i\tau H_{\sigma_x \sigma_x}}{2})\exp(\frac{-i\tau H_{\sigma_\alpha}}{2})
\end{split}
\end{equation}

with !(!should remove t in the braket)

%\begin{equation}
\begin{align}
\label{single_spin}
&\exp(\frac{-i\tau H_{\sigma_\alpha}}{2})= \exp(\frac{-i\tau(-\sum_{i=1}^{N}\sum_{\alpha=x,y,z}h^\alpha_i(t)\sigma^\alpha_i)}{2})\\
\label{double_spin_x}
&\exp(\frac{-i\tau H_{\sigma_x \sigma_x}}{2})=\exp(\frac{-i\tau(-\sum_{i=1}^{N}J^x_{ij}(t)\sigma^x_i\sigma^x_j)}{2})\\
\label{double_spin_y}
&\exp(\frac{-i\tau H_{\sigma_y \sigma_y}}{2})=\exp(\frac{-i\tau(-\sum_{i=1}^{N}J^y_{ij}(t)\sigma^y_i\sigma^y_j)}{2})\\
\label{double_spin_z}
&\exp({-i\tau H_{\sigma_z \sigma_z}})=\exp({-i\tau(-\sum_{i=1}^{N}J^z_{ij}(t)\sigma^z_i\sigma^z_j)})
\end{align}
%\end{equation}

, !where $h^\alpha_i(t), J^x_i(t), J^y_i(t)$, and $J^z_i(t)$ are the strength of Hamiltonian at the current time step according to Equation \ref{Hamiltonian_set}. Equation \ref{single_spin} can be further elaborated as 

\begin{equation}
\label{single_spin 2}
\exp(\frac{-i\tau(-\sum_{i=1}^{N}\sum_{\alpha=x,y,z}h^\alpha_i(t)\sigma^\alpha_i)}{2}) = \prod_{i=1}^{N}\exp(\frac{i\tau \sum_{\alpha_{x,y,z}}h^\alpha_i(t)\sigma^\alpha_i}{2}).
\end{equation}
 
The following equation set out a rotation of the vector $\mathbf{S}$ about the vector $\mathbf{v}$,

\begin{equation}
\label{single_spin 3}
!\exp(i\mathbf{v}\cdot \mathbf{S}) = \mathbb{I} cos(\frac{v}{2}) + \frac{2i\mathbf{v}\cdot \mathbf{S}}{v}sin(\frac{v}{2}),
\end{equation}

where $v$ is the norm of $\mathbf{v}$. With the aid of this equation, the matrix exponential of these single spin terms can be calculated analytically as 

\begin{equation}
\label{single_spin 4}
\exp(i\tau\mathbf{\sigma_i}\cdot\mathbf{h_i}) = \begin{pmatrix}
cos\frac{\tau h_i}{2}+\frac{ih^z_i}{h_i}sin\frac{\tau h_i}{2} & \frac{ij^x_i+h^y_i}{h_i}sin\frac{\tau h_i}{2}\\
\frac{ij^x_i-h^y_i}{h_i}sin\frac{\tau h_i}{2} & cos\frac{\tau h_i}{2}-\frac{ih^z_i}{h_i}sin\frac{\tau h_i}{2}
\end{pmatrix},
\end{equation}

where $h_i$ is the norm of the vector $\mathbf{h_i}=(h^x_i,h^y_i,h^z_i)$. It can be conclude from Equation \ref{single_spin 2}, \ref{single_spin 3}, and \ref{single_spin 4}, that the time evolution for single spin terms turns out to be a $2\times2$ matrix calculation and multiplication. !This can be done by picking right pairs of coefficient in the state vector and applying Equation \ref{single_spin 4}. \\

For the case of double spin operator, since the spin operator with different labels commute, Equation \ref{double_spin_x}, \ref{double_spin_y}, and \ref{double_spin_z} can be decomposed into 

\begin{align}
&\exp(\frac{-i\tau(-\sum_{i=1}^{N}J^x_{ij}(t)\sigma^x_i\sigma^x_j)}{2}) = \prod_{i,j=1}^{N}\exp(\frac{i\tau J^x_{i,j}(t)\sigma^x_i\sigma^x_i}{2})\\
&\exp(\frac{-i\tau(-\sum_{i=1}^{N}J^y_{ij}(t)\sigma^y_i\sigma^y_j)}{2}) = \prod_{i,j=1}^{N}\exp(\frac{i\tau J^y_{i,j}(t)\sigma^y_i\sigma^y_i}{2})\\
&\exp({-i\tau(-\sum_{i=1}^{N}J^z_{ij}(t)\sigma^z_i\sigma^z_j)}) = \prod_{i,j=1}^{N}\exp(i\tau J^z_{i,j}(t)\sigma^z_i\sigma^z_i).
\end{align}

The above expressions, likewise, can be determined analytically and yield 

\begin{align}
\label{double_spin_x 2}
&\exp(\frac{i\tau J^x_{i,j}\sigma^x_i\sigma^x_j}{2}) = \begin{pmatrix}
\cos(J^x_{i,j}\tau) & 0 & 0 & i \sin(J^x_{i,j}\tau) \\
0 & \cos(J^x_{i,j}\tau) & i \sin(J^x_{i,j}\tau) & 0 \\
0 & i \sin(J^x_{i,j}\tau) & \cos(J^x_{i,j}\tau) & 0 \\
i \sin(J^x_{i,j}\tau) & 0 & 0 & \cos(J^x_{i,j}\tau)  
\end{pmatrix} \\
\label{double_spin_y 2}
&\exp(\frac{i\tau J^y_{i,j}\sigma^y_i\sigma^y_j}{2}) = \begin{pmatrix}
\cos(-J^y_{i,j}\tau) & 0 & 0 & i \sin(-J^y_{i,j}\tau) \\
0 & \cos(J^y_{i,j}\tau) & i \sin(J^y_{i,j}\tau) & 0 \\
0 & i \sin(J^y_{i,j}\tau) & \cos(J^y_{i,j}\tau) & 0 \\
i \sin(-J^y_{i,j}\tau) & 0 & 0 & \cos(-J^y_{i,j}\tau)  
\end{pmatrix} \\
\label{double_spin_z 2}
&\exp(i\tau J^z_{i,j}\sigma^z_i\sigma^z_j) = \begin{pmatrix}
\exp(i J^z_{i,j} \tau) & 0 & 0 & 0 \\
0 & \exp(-i J^z_{i,j} \tau) & 0 & 0 \\
0 & 0 & \exp(-i J^z_{i,j} \tau) & 0 \\
0 & 0 & 0 & \exp(i J^z_{i,j} \tau).
\end{pmatrix} 
\end{align}

In a similar way, the time evolution of double spin operators can be evaluated by picking the 4 corresponding states, calculating the matrix, applying the matrix on $\ket{\Psi}$. Moreover, with these analytical expression (i.e. Equation \ref{single_spin 4}, \ref{double_spin_x 2}, \ref{double_spin_y 2}, and \ref{double_spin_z 2} ), it it now possible to carry out $\tilde{U}(\tau)$ in Equation \ref{tilde_U} and apply it to $\ket{\Psi(t)}$. Hence, the time evolution of the wave function can be calculated. \\

With these two algorithm, the full diagonalisation algorithm and the Suzuki-Trotter Product Formula algorithm, it is sufficient to simulate an quantum annealer at zero temperature, namely under ideal condition, on a classical computer. However, in order to simulate a quantum annealer at finite temperature, the computation process will be more complicated and a simple implementation of these two algorithm will not be practical. Further detail will be presented in the next section. \\

%Break the H to small operation that operate on the state. According to $J^x , J^y, and Jz; h^x, h^y, and h^z$, we need to find the pair or quad pair for the $\Psi$
%
%$\circ \circ \circ ,0, \circ$
%
%$\circ \circ \circ ,1, \circ$
%
%$\circ ,0, \circ ,0, \circ$
%
%$\circ ,0, \circ ,1, \circ$
%
%$\circ ,1, \circ ,0, \circ$
%
%$\circ ,1, \circ ,1, \circ$

\subsection{Algorithm for system with Temperature Effect}
%Here since we have in total 16 spins, of course we cannot use product formula algorithm. But we because have temperature term now, which means we may run over several states, therefore the product formula algorithm is not enough for use now. Therefore, we use random sampling method.

\subsubsection{Canonical-Thermal State}
%\subsubsection{Boltzmann Distribution/ Assemble Average}

!In order to simulate a system at finite temperature, the quantum subsystem S is coupled to a quantum environment E and the Hamiltonian of the entire system (i.e. S+E) is defined as

\begin{equation}
H = H_S + H_E + GH_{SE},
\end{equation} 

where $H_S$ is the Hamiltonian of the subsystem, $H_E$ is the Hamiltonian of the environment, and $H_{SE}$ is the interaction between the subsystem S and the environment E with $G$ indicates the global coupling strength between S and E. \\

!The quantum subsystem S and the environment E can be described respectively by density matrix 
\begin{equation}
\begin{split}
\rho_S = \ket{S'}\bra{S'}\\
\rho_E = \frac{e^{-\beta H_E}}{Z}, 
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\langle A(t) \rangle &= \bra{\Psi(t)}A\ket{\Psi} \\
&=Tr A(t)\rho(0), A(t)=e^{iHt}Ae^{-iHt}=U^\dagger AU(t)\\
&=Tr A \rho(t), \rho(t)=U^\dagger \rho U(t)\\
\rho(0)=\rho_S\rho_E, \rho(t)=\ket{\Psi(t)}\bra{\Psi(t)}, \ket{\Psi(t)}= U(t)\ket{\Psi(0)}\\
\end{split}
\end{equation}


!!where $S'$ is QQ, $\beta$ is the thermodynamic beta defined as $\frac{1}{k_b T}$,  and Z is partition function defined as $Z = \sum_i e^{-\beta E_i}$. \\



???	By adding environment, the temperature will affect how many state should we calculate
We still use product formula. and we use assemble average to calculate each initial state exactly.
\subsubsection{Random wave function}
Reference check! II Theory part.
\cite{Hams2000} 







\section{Quantum Annealer Simulation: Ideal Case}
\label{result_ideal}
	There is no ideal system. Environment will always affect the subsystem. However, I start from a simple case, that is a subsystem without environment effect. 
\subsection{??Simulation Set up}
	8 spin without environment spin. With linear time scheme. 

\subsection{Result}
\subsection{the evolution of the spin, energy, and success probability during the annealing}
	A general picture on the annealing behaviour. The expectation value of spin should start from x direction and end up in z direction. During the process y may only have some fluctuation. For the energy of subsystem we expect it to first increase and then decrease to a lower level. We knew the ground state ready, so in the end we expect to see the ground state evolute form 0 to 1, if this is a successful annealing.
	\begin{itemize}
		\item \checkmark Figure here: The system energy vs. lambda
		\item \checkmark Figure here: The spin x vs. lambda
		\item \checkmark Figure here: The spin z vs. lambda
		\item \checkmark Figure here: The success probability vs. lambda
	\end{itemize}
	\begin{figure}[h!]
		\centering
		\includegraphics[height=300pt]{result_picture/energy_evolution_full_diag/Figure_EnergyvsLambda.png}
		\caption{Insert caption here.}
		\label{example_figure}
	\end{figure}
	\begin{figure}[h!]
		\centering
		\includegraphics[height=250pt]{result_picture/probability_spin_system/Figure_Annealing_spin.png}
		\caption{Insert caption here.}
		\label{example_figure}
	\end{figure}
	\begin{figure}[h!]
		\centering
		\includegraphics[height=300pt]{result_picture/probability_spin_system/Figure_Annealing_probability.png}
		\caption{Insert caption here: Success Probability vs. $\lambda$.}
		\label{example_figure}
	\end{figure}
\subsubsection{The Effect of step size tau}
	After having a look at the general properties. Now I try to investigate the size of time step. When the Hamiltonian is time-dependent. The tau should be small enough.  other wise the result would deviate. 
	\begin{itemize}
		\item \checkmark Figure here: compare the result of product formula with the result of full diagonalization: success probability vs. annealing Time with different tau.
	\end{itemize}
	\begin{figure}[h!]
		\centering
		\includegraphics[height=300pt]{result_picture/compare_full_product/Figure_product_formula.png}
		\caption{Insert caption here.}
		\label{example_figure}
	\end{figure}
	\begin{figure}[h!]
		\centering
		\includegraphics[height=300pt]{result_picture/compare_full_product/Figure_full_diag.png}
		\caption{Insert caption here.}
		\label{example_figure}
	\end{figure}
\subsubsection{The Effect of Annealing Time}
	Setting up the proper $\tau$. Then I start to simulate the annealing process with different annealing time. The annealing should be long enough to find a ground state. Also I compare the result of full diagonalization and product formula algorithm. 
	\begin{itemize}
		\item Figure here: compare the result of different annealing time of full diagonalization: success probability vs.lambda with different annealing time
		\item \checkmark Figure here: compare the result of different annealing time of full diagonalization: success probability vs.lambda with different annealing time
		\item \checkmark Figure here: compare the result of different annealing time of full diagonalization: energy vs.lambda with different annealing time
		\item \checkmark Figure here: compare the result of different annealing time of full diagonalization: energy vs.lambda with different annealing time
	\end{itemize}
\subsubsection{The Effect of Minimum Gap}
	According to Landau-Zener theorem, the successful probability depend on the gap in between the ground state and the first excited state.
	\begin{itemize}
		\item \checkmark Figure here: energy spectrum: the energy for all states vs. lambda
		\item \checkmark Figure here: a close view of the result above: the energy for lowest 20~30 states vs. lambda
		\item \checkmark Figure here: compare: the success probability under the annealing time that maximise the minimum gap difference vs. minimum gap value 
	\end{itemize}
		\begin{figure}[h!]
			\centering
			\includegraphics[height=300pt]{result_picture/energy_evolution_product_formula/with_product_formula/Figure_energy_spectrum.png}
			\caption{Insert caption here.}
			\label{example_figure}
		\end{figure}
		
\newpage
	
\section{Quantum Annealer Simulation with Temperature Effect}
\label{result_temp}
	After the case without environment, now we move on to the one with environment.
\subsection{??Simulation Set up}
	Here the system consist of a 8-spins subsystem and 8-spins environment. The interaction depend on the coupling factor. When factor is 0, the subsystem cannot be affected by the environment at all. On the other, if the factor is 1, the spin of environment may fully interact with subsystem just as one of the spin inside.

\subsection{Result}
\subsubsection{The Effect of Temperature}
	\begin{itemize}
		\item Figure here: display plots with different coupling factor: success probability of a coupling factor with different temperature vs. annealing Time
	\end{itemize}
\subsubsection{The Process toward Quasi-static}
	See Fig 3 of this reference	\cite{Amin2015} 
	
	In the beginning of the annealing. The probability of finding the ground state may still increase, because the environment haven't yet to affect the subsystem. Then when the environment start interacting with subsystem. After it equilibrium with the subsystem. It will continuous to anneal and the probability will increase. 
	\begin{itemize}
		\item \checkmark Figure here: display plots with different temperature: success probability with different coupling factor vs. annealing time
		
		\item \checkmark Figure here: repeat above plots with Random wave function
	\end{itemize}
	
		\begin{figure}[h]
			\centering
			\includegraphics[height=300pt]{result_picture/JvsT_plot/T1/JvsTwTemp1.png}
			\caption{Insert caption here.}
			\label{example_figure}
		\end{figure}
	
\subsubsection{The Effect of Annealing Time}
	This subsection may be removed 
\subsubsection{The Effect of Minimum Gap}
	So the scatter effect come from the spin numbering. When the coupling factor become larger and larger. The sccatering effect become more clear. On the other hand, Temperature is not the critical reason to this phenomenon. But the temperature will also enlarge the scatter. The temperature has an influence on how many environment state are part of the evolution. If the temperature is low the revolution only in the ground state. When temperatrue is high, almost all environment state is part of the evolution. the state is average out by the environment states. So the scatter of the problem hamiltonian is not clear. Also since we are in an effective temperature unit. delta gap/Temperature. Therefore, when temperature is larger than gap, they are in excited state. so the annealing is not clear and cant find the ground state. vice versa. when the temperature is smaller than 1. we can see the annealing effect and probability to find the ground state. 
	\begin{itemize}
		\item \checkmark Figure here: compare: the success probability under the annealing time that maximise the minimum gap difference vs. minimum gap value.
	\end{itemize}
		\begin{figure}[h!]
			\centering
			\includegraphics[height=350pt]{result_picture/gap_result_environment/Figure_PvsGap_T2e2_Temp1.png}
			\caption{Insert caption here.}
			\label{example_figure}
		\end{figure}
\section{??D-wave Practice}
	\cite{Johnson2011}
	D-wave use of super conducting qubit
	
\section{??Parallelization}
	\definecolor{light-gray}{gray}{0.4}
	\textcolor{light-gray}{Shoul I put the result of openMP parallerization?}
	It is obvious that i cant put the parallization in between the different time step, because they depend on each other. So i put openmp in side each step for the state. also because the calculation may cause race condition, i have to put it in the second loop instead of the most outside one. this may decrease the optimisation of the running time. However, i think this is the moderate way to get a balance of not getting code too complicate and too slow running time.
	
	! \textcolor{red}{The runtime difference for a single run with core 1,2,4,8,16,32,64,128.}

	
\section{Applicatoin on Machine Learning / Simulation}
	
	\cite{Adachi2015}
	\cite{Benedetti2016}
	\cite{Boyda2017}
	\cite{OMalley2017}
	\cite{Potok2017}
	
	
	Application
	 
	-tree recognition
	
	-hand writing
	
	-face detection
	


\section{Conclusion}
\section{miscellaneous}




Please specify your name, matriculation number, name of advisor and the title of your report in \linebreak
\verb+titlepage.tex+.
The title page will not count for the 20 pages.

Using bibtex you can cite in an organized way and without much work.
Just enter the information about a paper or an article you want to cite in the \verb+seminar_report.bib+ file and use \verb+\cite+ to cite them. For example \cite{Author08CVPR},\cite{Author04IJCV}.
Don't forget to compile the bib file and Latex will add all the cited references at the end.
Cite all the literature you use and state where figures are from!

I am a section. Latex will give me a number \emph{automatically} and put me into the table of contents.
Using \verb+\label+ and \verb+\ref+ you can use Latex to write that this is Section \ref{section}.



\subsection{a subsection}
I am a subsection.

\begin{itemize}
	\item I am an item.
	\item [-] I am another item.
\end{itemize}

\subsubsection{a small subsection}
I am a subsubsection, an even smaller subsection.

\begin{tabular}{|l|c|}
\hline
I am a tabular & with two columns. \\
\hline
The left column is aligned left & and the right columns is centered. \\
\hline
\end{tabular}


\begin{figure}[h]
\centering
\includegraphics[height=100pt]{doge.jpeg}
\caption{Insert caption here. Image from \cite{lenna}. }
\label{example_figure}
\end{figure}
Figure \ref{example_figure} also gets a number automatically and will be placed where Latex thinks it looks good. You can specify a preference with h(ere), t(op), b(ottom), p(age).


%\begin{figure}
%%	\begin{center}
%		\resizebox{!}{!}{\input{pvsgap}}
%%	\end{center}
%\end{figure}

Latex is also really good at printing equations: $E=mc^2$

\begin{equation}
A = \sum_{i=1}^N A_1 \cdot A_2
\end{equation}

\subsubsection{Possible Material}
Result on spin and Energy env,sys,se

success probability of ground state

spin system

energy spectrum

evolution of spin and energy

Runtime comparison

Parallel possibility

-Introduction

-Annealing Theorem

-Full-diagonalization

-suzuki-trotter product formula

-size of tau

-The effect of annealing Time w/ w/o Heat bath

-The effect of minimum gap w/ w/o Heat bath

-Landau-Zener Theorem

-2-SAT problem 

-Random wave function  

-The effect of heat bath(Coherent - Transverse - quasiequilibrium) at different temperature

D-wave practice

-Hamiltonian Mapping

-Annealing Time scheme

-Time-Dependent Schrï¿½dinger Equation

-Boltzmann distribution + trace of the observable


% +++++++++++++++++++++++++

% =========================================================================
\bibliographystyle{alpha}
\bibliography{master_report}

% =========================================================================

\end{document}
